{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c1109d",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "Note: our lab 2 is the lecture lab 3\n",
    "\n",
    "Tha aims of the lab are to:\n",
    "- model the probability of generating language with unigrams and trigram LMs\n",
    "- evaluate the quality of the language model using perplexity \n",
    "- understand and address the issues of sparsity in language modeling\n",
    "- experiment with applications of language models in understanding text\n",
    "- \n",
    "\n",
    "bigram model -> one character predicts the next one with a lookup table of counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715104c1",
   "metadata": {},
   "source": [
    "## Background: Load Linkedin\n",
    "run the cells below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33652f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path) -> pd.DataFrame:\n",
    "  return pd.read_csv(file_path)\n",
    "\n",
    "def get_text_as_string(content:pd.Series):\n",
    "  return content.str.cat(sep=\" \")\n",
    "\n",
    "blokedin_dataset_path = Path(\".\").resolve().parent / \"datasets\" / \"messages.csv\"\n",
    "print(blokedin_dataset_path, type(blokedin_dataset_path))\n",
    "blokedin_dataset = load_dataset(blokedin_dataset_path)\n",
    "blokedin_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce7070",
   "metadata": {},
   "source": [
    "Before we get the text as a single string, we need to add the special token `<S>` at the beginning and end of each message so that the n-gram model is able to generate messages that replicate the dataset corpus.\n",
    "\n",
    "To do this, we need to use a .map function possibly on the df.content Series.\n",
    "\n",
    "We're not putting the `<s>` at the end. What is the effect of that in our n-gram language model?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#blokedin_dataset[\"content\"] = \"<s>\" + blokedin_dataset[\"content\"] #+ \"<s>\"\n",
    "# alternative\n",
    "# blokedin_dataset[\"content\"] = blokedin_dataset[\"content\"].map(lambda message: f\"<s> {message}<s>\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ae337",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blokedin_dataset[\"content\"].iloc[0])\n",
    "print(blokedin_dataset[\"content\"].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this to generate the character's vocab from the content already in our corpus\n",
    "content = get_text_as_string(blokedin_dataset[\"content\"])\n",
    "print(len(content))\n",
    "\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fdd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(content)))\n",
    "content_chars = set(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad1536",
   "metadata": {},
   "source": [
    "Let's keep the `\\\\n` in the original text so that our n-gram model can detect that in its language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23620187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f04dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(string.printable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdca2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "printable_ch_not_in_content = [ch for ch in string.printable if ch not in content_chars]\n",
    "printable_ch_not_in_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d565d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "printable_characters_in_content = [ch for ch in string.printable if ch in content_chars]\n",
    "printable_characters_in_content\n",
    "\"-\" in printable_characters_in_content\n",
    "print(len(printable_characters_in_content), len(string.printable), len(printable_ch_not_in_content))\n",
    "content_chars_not_in_printable = [ch for ch in content_chars if ch not in string.printable]\n",
    "print(content_chars_not_in_printable)\n",
    "print(len(content_chars_not_in_printable))\n",
    "print(\"-\" in string.printable)\n",
    "print(\"-\" in content_chars_not_in_printable)\n",
    "print(\"-\" == content_chars_not_in_printable[11], content_chars_not_in_printable[11], ord(content_chars_not_in_printable[11]), ord(\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = \"<s>\"\n",
    "#chars = [special_token] # we'd like to keep this at the beginning\n",
    "base_vocab = set(string.printable) # i.e. ascii 32 -> 127\n",
    "assert len(base_vocab) == len(string.printable), f\"the base vocab size is not the same as the printable characters count\"\n",
    "\n",
    "chars = sorted(set(content).union(base_vocab)) # returns a list\n",
    "\n",
    "printable_chars_not_in_chars = [ch for ch in string.printable if ch not in chars]\n",
    "\n",
    "\n",
    "assert all(char in chars for char in string.printable) == True, f\"not all the printable characters are in the chars list {printable_chars_not_in_chars}\"\n",
    "\n",
    "stoi = {char:i+1 for i, char in enumerate(chars)} # I think we should be sorting the vocabulary and add the starting char\n",
    "stoi[special_token] = 0\n",
    "itos = {i:char for char,i in stoi.items()} # this is equivalent to the above\n",
    "# vocabulary = set(content)\n",
    "\n",
    "# print(len(vocabulary))\n",
    "# print(vocabulary)\n",
    "\n",
    "\n",
    "# print(len(vocabulary))\n",
    "# vocabulary = sorted(vocabulary) # sorted returns a list\n",
    "\n",
    "print(special_token in chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just kept the old version to see that you could use another approach too\n",
    "#vocabulary = {char:i for i, char in enumerate(vocabulary)}\n",
    "\n",
    "# we need to make sure that all the alphabet chars and numbers are in the vocabulary\n",
    "# atoi = {chr(i):i for i in range(32, 126)}\n",
    "# for char, i in atoi.items():\n",
    "#   if char not in vocabulary:\n",
    "#     print(f'char={char} not in vocabulary')\n",
    "#     vocabulary.append(char)\n",
    "\n",
    "# vocabulary.append(special_token)\n",
    "# print(vocabulary)\n",
    "\n",
    "#itos = {i:char for i, char in enumerate(vocabulary)} # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stoi)\n",
    "print(itos)\n",
    "print(len(stoi))\n",
    "print(stoi[special_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347df0d",
   "metadata": {},
   "source": [
    "Question: why did we put the `<s>` in the end? Could have we put it at the start? what would have changed? Also, why do we only have one special character and not one for the end of text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f21ec",
   "metadata": {},
   "source": [
    "So the vocab size is 155 with the special_token `<s>` added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = blokedin_dataset[\"content\"]\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I think here we can use the Counter() provided by python\n",
    "from collections import defaultdict\n",
    "b = defaultdict(int) # named the same as https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part1_bigrams.ipynb\n",
    "\n",
    "for message in content:\n",
    "  characters = [special_token] + list(message) + [special_token]\n",
    "  for ch1, ch2 in zip(characters, characters[1:]):\n",
    "    #print(bigram)\n",
    "    bigram = (ch1, ch2)\n",
    "    #print(bigram)\n",
    "    b[bigram] = b.get(bigram, 0) + 1\n",
    "    #b[bigram] += 1 # I prefer this\n",
    "\n",
    "\n",
    "# splittt = content.str.split()\n",
    "# print(splittt)\n",
    "# type(splittt)\n",
    "\n",
    "Vocab_counter = dict(sorted(b.items(), key=lambda items: items[1], reverse=True))\n",
    "print(Vocab_counter)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this counts the frequencies of each character occuring in our corpus\n",
    "# create the bigram frequencies\n",
    "# for message in content.:\n",
    "#   Vocab_counter[char] += 1\n",
    "\n",
    "# Vocab_counter = dict(sorted(Vocab_counter.items(), key=lambda items: items[1], reverse=True))\n",
    "# Vocab_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1e7a8",
   "metadata": {},
   "source": [
    "Now we have a bigram counter of frequencies and we need the model to generate the text based on these frequencies. Like create probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca01b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are two approaches here right, create a matrix and or\n",
    "# in the original karpathy used a 27x27 matrix, we have a bigger vocab size\n",
    "n = len(chars) + 1\n",
    "#N = torch.zeros((n, n), dtype=torch.int32) # why did we choose int32?? could have we chosen something unsigned?\n",
    "N = torch.zeros((n, n), dtype=torch.int32) # why did we choose int32?? could have we chosen something unsigned?\n",
    "\n",
    "# now for each bigram occurence in the lookup table b we need to calculate the probability\n",
    "for message in content:\n",
    "  tokens = [special_token] + list(message) + [special_token]\n",
    "  for ch1, ch2 in zip(tokens, tokens[1:]):\n",
    "    idx_ch1 = stoi[ch1]\n",
    "    idx_ch2 = stoi[ch2]\n",
    "    #print(idx_ch1, idx_ch2)\n",
    "\n",
    "    N[idx_ch1, idx_ch2] += 1\n",
    "    #print(N[idx_ch1, idx_ch2])\n",
    "\n",
    "N[0, 46]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd855f80",
   "metadata": {},
   "source": [
    "Let's display this N frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b791509",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(itos[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5dc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap=\"Blues\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f074f92",
   "metadata": {},
   "source": [
    "We can immediately notice that the matrix is very sparse and the majority of occurrences are at the center from more or less index 58 to index 100 where if you have a good eye you can relate that range to the ascii prinatble charactes in the alphabet. This should'n be a suprise as we're dealing with text messages written in english.\n",
    "\n",
    "We can also see a lot of occurrences in the first few rows, specifically row 6 whic if you look at the itos you notice it's a space and this shouldn't be a surprise either as the space in our messages is used a lot. Now, how do we deal with the space when we sample with this model? will we have very short words? Let's see.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f45793",
   "metadata": {},
   "source": [
    "Let's look at the values at row 6, the one with the space followed by a character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f8c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "N[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085f4bc",
   "metadata": {},
   "source": [
    "Now let's calculate the probabilities of that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a40f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = N[6].float()\n",
    "#print(p)\n",
    "p = p / p.sum()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc6f04",
   "metadata": {},
   "source": [
    "Let's visualize the probabilities for the space followed by a character. What distribution do we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,3))\n",
    "plt.bar(range(len(p)), p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a872c",
   "metadata": {},
   "source": [
    "Now let's sample from the probabilites using multinomial.\n",
    "\n",
    "Question, how does the multinomial work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b708ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a generator for reproducibility\n",
    "g = torch.Generator().manual_seed(10000)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "print(ix)\n",
    "print(itos[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.rand(3, generator=g)\n",
    "print(p)\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10da845",
   "metadata": {},
   "outputs": [],
   "source": [
    "genmultinomial = torch.multinomial(p, num_samples=100, replacement=True, generator=g)\n",
    "genmultinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(genmultinomial)), genmultinomial)\n",
    "plt.show()\n",
    "# this should represent the same as the p from torch.random(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05581847",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = torch.zeros((n, n), dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc313634",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2945abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrue = P.sum(1, keepdim=True)\n",
    "print(ptrue)\n",
    "print(ptrue.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 160, 160\n",
    "# 160,   1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade40217",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 160, 160\n",
    "#   1, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181815ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    p = P[ix]\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0: break\n",
    "  print(''.join(out)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa16ac",
   "metadata": {},
   "source": [
    "Now, this model is really bad, how do we improve it?\n",
    "\n",
    "- maybe just use printable ascii characters and remove the emojiis occurrences. \n",
    "- what is the probability of these before we model it\n",
    "- would a trigram perform better??\n",
    "- what is the error (entropy??) of the text before we start modeling this?\n",
    "\n",
    "\n",
    "Let's implement 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d02ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b0de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
