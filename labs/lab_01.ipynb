{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "Tha aims of the lab are to:\n",
    "- [ x ] implement a simple tokenizer (just consecutive characters that form words) and compare it to the nltk treebank word tokenizer\n",
    "- [] extract the counts of the vocabulary and the tokenizer\n",
    "- [ x ] implement a simple Porter stemmatizer and compare it to the nltk one and see if there are differences\n",
    "- [ ] implement the BPE tokenizer\n",
    "- [ ] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process Linkedin messages data\n",
    "\n",
    "This is the data of messages received via Linkedin collected manually and put into a json structure in a python file and processed into a csv file via the script at `scripts/process_linkedin_messages.py`.\n",
    "\n",
    "The csv has four columns:\n",
    "- `block`: a binary `0` or `1` indicating whether we should block the sender or not\n",
    "- `content`: conataining the content of the message received.\n",
    "- `subject`: contains the subject of the message received\n",
    "- `has_attachment`: a binary `0` or `1` indicating whether the message received contained an attachment\n",
    "\n",
    "\n",
    "To follow along you could recreate a similar structure and run it through the `scrips/process_linkedin_messages.py` to create your dataset in a csv file format. We'll use that csv file and load it into a Pandas dataframe to do other NLP tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(corpus_filepath):\n",
    "  dataset = pd.read_csv(corpus_filepath)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_corpus_filepath = \"../datasets/messages.csv\"\n",
    "\n",
    "messages_dataset = load_corpus(messages_corpus_filepath)\n",
    "\n",
    "print(f\"total messages in the dataset: {len(messages_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded our messages dataset, let's analyze the data.\n",
    "\n",
    "#### Your task: \n",
    "Use the head() function to print out the top few samples of the Linkedin messages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the messages do not contain the subject and they are represented as `NaN` (Not a Number) value in the cell.\n",
    "\n",
    "TODO in the future we'd need to deal these in some way when using this column as a feature in our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print a statistical summary of the data using `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_dataset.describe(include=\"all\").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we see? Do most messages contain the subject? Are all the subjects unique? Do most of the messages contain an attachment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration\n",
    "\n",
    "To iterate through DataFrame's row in pandas one can use:\n",
    "\n",
    "- DataFrame.iterrows() or DataFrame.itertuples()\n",
    "\n",
    "*Question* Which is faster? \n",
    " - We will time them using timeit\n",
    " - We will demonstrate how to use iteration with a \"for comprehension\".\n",
    " \n",
    "#### Your Task\n",
    "Execute the code below and read it to understand how it works. In particular, notice the selection of fields from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the use of itertools which has many useful iteration utilities.\n",
    "from itertools import islice\n",
    "\n",
    "N = 5\n",
    "\n",
    "# print the first N rows using both methods\n",
    "# iterrows\n",
    "for index, row in islice(messages_dataset.iterrows(), N):\n",
    "  print(index, row[\"content\"], row[\"block\"])\n",
    "\n",
    "# itertuples\n",
    "print(f\"now itertuples\")\n",
    "for row in islice(messages_dataset.itertuples(index=True, name=\"Pandas\"), N):\n",
    "  print(getattr(row, \"content\"), getattr(row, \"block\"))\n",
    "\n",
    "\n",
    "N = len(messages_dataset) # I don't have a bigger dataset, we could use the imdb in case\n",
    "# iterrows\n",
    "time1 = %timeit [row[\"content\"] for index, row in islice(messages_dataset.iterrows(), N)]\n",
    "\n",
    "time2 = %timeit [getattr(row, \"content\") for row in islice(messages_dataset.itertuples(index=True, name=\"Pandas\"), N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `itertuples` is faster by some margin.\n",
    "\n",
    "\n",
    "We can index into the dataframe, by asking for particular rows:\n",
    "- `iloc` allows us to ask for particular row(s) indexed by (integer) position\n",
    "- `loc` allows us to ask for particular row(s) indexed by a label \n",
    "\n",
    "By default labels are automatically assigned, starting from 0, so the two statements are indentical for us.\n",
    "\n",
    "Each row in the dataframe is a Series type, a one dimentional labelled array holding any data type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(messages_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages_dataset.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages_dataset.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(messages_dataset.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn more about the Linkedin Messages collection statistics. \n",
    "\n",
    "#### Your Task\n",
    "Use count() on the `messages_dataset` object to print a count distribution. \n",
    "- Note: count() gives count values for each column in the frame independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have around 103 messages (depends on how many you collected).\n",
    "- Do all columns have the same count? What does this tell us about the collection?\n",
    "\n",
    "Now, let's dig and explore more statistics on the messages by their text content.\n",
    "\n",
    "First, let's select all the values of the *text* column and inspect its type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_contents = messages_dataset[\"content\"]\n",
    "print(type(messages_contents))\n",
    "messages_contents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a Series, a row in a DataFrame. We will use a useful function on the Series object, `value_counts` to group and count the values for the `content` series.\n",
    "\n",
    "#### Your task:\n",
    "Use value_counts on the `messages_contents` variable.\n",
    " - Print a statistical summary of the data using .describe() \n",
    " - Use head() to print out the top 5 subreddits with their count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_counts = messages_contents.value_counts()\n",
    "messages_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Since all our messages are unique we'll see the count of 1 for each message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What information is provided by the describe() function?\n",
    "- What does the statistical summary tell you about the frequency distribution of Linkedin Messages?\n",
    "\n",
    "TODO~~Consider what was discussed in lecture 2 about word distributions; this distribution follows a similar pattern. This is typical of real-world text data and will have important ramifications later in the course.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_counts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get some more interesting statistics using the value_counts function.\n",
    "\n",
    "What is the percentage of the messages that were blocked and the ones not blocked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_messages = messages_dataset[\"block\"]\n",
    "blocked_messages_counts = blocked_messages.value_counts()\n",
    "blocked_messages_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_messages_counts.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO confirm these questions make sense\n",
    "- What does the `blocked_messages_counts` variable represent?\n",
    "- Critically look at these statistics\n",
    "- ~~What is the shortest message, longest message~~\n",
    "- ~~How many posts are in a 'typical' message~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_blocked_messages_percentage = blocked_messages_counts[0] / blocked_messages_counts.sum()\n",
    "\n",
    "print(f\"percentage of non blocked messages in the messages dataset: {non_blocked_messages_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_messages_percentage = blocked_messages_counts[1] / blocked_messages_counts.sum()\n",
    "print(f\"percentage of blocked messages in the messages dataset: {blocked_messages_percentage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing of Linkedin messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take our first steps processing the Linkedin messages as text. We will apply an NLTK text processing pipeline to the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "[NLTK](http://www.nltk.org/) is a large compilation of Python NLP packages. It includes implementations of a number of classic NLP models, as well as utilities for working with linguistic data structures, processing text, and managing corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we may use spaCy.\n",
    "\n",
    "Let's import the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tokenization\n",
    "We will try tokenization ourselves using NLTK's tokenizers. You may find the documentatioon of the [tokenize package](https://www.nltk.org/api/nltk.tokenize.html) informative.\n",
    "\n",
    "\n",
    "#### Your task\n",
    "\n",
    "Tokenize the body field of a message using Regular Expression and Treebank tokenizers (recall this as standard tokenizer from Penn Treebank discussed in lecture 1) and compare them:\n",
    "\n",
    "1. Import the Reg Exp Tokenizer from NLTK.\n",
    "2. Create a regular expression tokenizer that uses a simple pattern -- a sequence of one or more \"word characters\".\n",
    "3. Create a tokenizer that tokenizes using the (Penn) Treebank Word Tokenizer. Find the right tokenizer in the package documentation.\n",
    "4. Tokenize the sample message below using each of the tokenizers and print the resulting tokens for each.\n",
    "5. Inspect and compare the output of the tokenizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sample message to tokenize\n",
    "text = messages_dataset.iloc[69]['content']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk tokenizer\n",
    "def nltk_tokenizer(text, regex=None):\n",
    "  regex = regex or r\"\\w+\"\n",
    "  return RegexpTokenizer(regex).tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def my_tokenizer(text):\n",
    "  pattern = r\"\\w+\"\n",
    "  return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "def treebank_word_tokenizer(text):\n",
    "  tokenizer = TreebankWordTokenizer()\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens = nltk_tokenizer(text)\n",
    "my_tokens = my_tokenizer(text)\n",
    "treebank_tokens = treebank_word_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(treebank_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization matters\n",
    "Tokenization is a critical first choice in developing text applications.  Below are some questions to consider when comparing the tokenizers.\n",
    "\n",
    "- What are the key differences between the tokenizers?\n",
    "- How do they treat punctuation?\n",
    "- What happens to the link + URL?\n",
    "- What is a 'good' vs a 'bad' tokenizer?  \n",
    "- How would you critically select one to use in an application? \n",
    "\n",
    "Are either of these tokenizers perfect? Consider how you would change the tokenizer to make it effective for the Reddit data. Or for the Linkedin Messages data.\n",
    "\n",
    "\n",
    "\n",
    "- what difference do we see between the simple tokenizer and the treebank word tokenizer?\n",
    "\n",
    "- what are the consequences of these differences?\n",
    "\n",
    "- when do we use the simple tokenizer and the treebank word tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Tokenizer\n",
    "\n",
    "Now, let's also introduce the BPE tokenizer implemented in lecture 1 and apply it to the `text`. We'd need to re-train it on the entire messages.\n",
    "\n",
    "TODO\n",
    "- In our Linkedin messages let's replace the links with the text `[URL]` using regex substitutions. We could even replace it with LINK and see what is the difference.\n",
    "- eventually compare it with sentencepiece and tiktoken from openai just to see what modern tokenizers do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE on Words Algorithm rather than on chars??? reread the first chapter and follow the lab and the lecture and see where are the differences\n",
    "- begin with a vocabulary that is all the individual characters\n",
    "- examine training corpus. (what does this mean???), I think this is related to below\n",
    "- choose the two symbols that are most frequently adjacent, e.g. 'A' and 'B'\n",
    "- add the merged symbol 'A' and 'B' with 'AB'\n",
    "- continue to count and merge creating longer and longer characters strings, until *k* merges have been creating *k* novel tokens.\n",
    "- *k* thus becomes the parameter of the algorithm\n",
    "- the resulting vocabulary consists of the original characters plus *k* new symbols\n",
    "- the newly created tokens are the tokens that should be matched first, longest first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows is a pandas Series\n",
    "def extract_corpus_text_from_pd_rows(rows) -> str:\n",
    "  corpus_text = []\n",
    "  for row in rows:\n",
    "    corpus_text.append(row)\n",
    "  output = \"\".join(corpus_text)\n",
    "  assert isinstance(output, str), f\"output is not of type str, got {type(output)}\"\n",
    "  return output\n",
    "\n",
    "\n",
    "def create_initial_vocabulary(corpus_text:str):\n",
    "  unique_chars = set()\n",
    "  for c in corpus_text:\n",
    "    if c not in unique_chars: unique_chars.add(c)\n",
    "  \n",
    "  # just some asserts\n",
    "  alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "  for c in alphabet:\n",
    "    if c not in unique_chars:\n",
    "      raise f\"character {c} not in the unique vocabulary\"\n",
    "\n",
    "  for n in range(10):\n",
    "    if str(n) not in unique_chars:\n",
    "      raise f\"number {n} not in the unique vocabulary\"\n",
    "  return unique_chars\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_most_frequent_tokens(tokenized_corpus_text):\n",
    "  counts = defaultdict(int)\n",
    "  for word_token in tokenized_corpus:\n",
    "    for pair in zip(tokens[])\n",
    "    counts[tokens] += 1\n",
    "\n",
    "  sorted_counts = list(sorted(counts.items(), key=lambda item: item[1], reverse=True))\n",
    "  print(sorted_counts)\n",
    "  return sorted_counts[0]\n",
    "\n",
    "# we need tokenized training_corpus, otherwise this won't work\n",
    "def update_training_corpus(training_corpus, token):\n",
    "  updated_training_corpus = []\n",
    "  for i, c in enumerate(training_corpus):\n",
    "    if i < len(training_corpus) - 1 and c + training_corpus[i+1] == token:\n",
    "      updated_training_corpus.append(token)\n",
    "      i += 1\n",
    "    else:\n",
    "      updated_training_corpus.append(c)\n",
    "  return \"\".join(updated_training_corpus)\n",
    "\n",
    "def add_new_token_to_vocabulary(V, new_token):\n",
    "  if new_token not in V: V.add(new_token)\n",
    "\n",
    "\n",
    "\n",
    "messages_corpus_text = extract_corpus_text_from_pd_rows(messages_dataset[\"content\"])\n",
    "print(messages_corpus_text)\n",
    "print(f\"len of the corpus is {len(messages_corpus_text)}\")\n",
    "V = create_initial_vocabulary(messages_corpus_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bpe(corpus, k_merges=1):\n",
    "  merges = []\n",
    "  tokenized_messages_corpus_text = [list(ch) for ch in messages_corpus_text]\n",
    "  vocabulary = create_initial_vocabulary(tokenized_messages_corpus_text)\n",
    "\n",
    "  for k in k_merges:\n",
    "    most_f = get_most_frequent_tokens(messages_corpus_text)\n",
    "    print(most_f)\n",
    "    new_token = most_f[0][0] + most_f[0][1]\n",
    "    print(f\"new_token={new_token}, len(new_token)={len(new_token)}\")\n",
    "\n",
    "    merges.append(new_token)\n",
    "\n",
    "    add_new_token_to_vocabulary(V, new_token)\n",
    "    print(V)\n",
    "\n",
    "    messages_corpus_text = update_training_corpus(messages_corpus_text, new_token)\n",
    "    print(messages_corpus_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bpe(messages_corpus_text, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to replace any link occurrence with the string `URL` or `LINK` or `[LINK]`, depending on the which one someone prefers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk tokenizer with fixed regular expression\n",
    "def nltk_tokenizer(text):\n",
    "  regex_pattern = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\d+| ?[^\\sA-Za-z\\d]+|\\[URL\\]|\\S+\"\"\"\n",
    "  return RegexpTokenizer(regex_pattern).tokenize(text)\n",
    "\n",
    "messages_corpus_filepath = \"../datasets/messages.csv\"\n",
    "messages_dataset = load_corpus(messages_corpus_filepath)\n",
    "\n",
    "print(f\"messages_dataset={len(messages_dataset)}\")\n",
    "\n",
    "messages_tokens = [] # use this to create another dataframe with the tokens? or just the text actually as one string\n",
    "text = []\n",
    "list_messages = []\n",
    "for row in messages_dataset.itertuples(index=True, name=\"Pandas\"):\n",
    "  message = getattr(row, \"content\")\n",
    "  text.append(message)\n",
    "  # sub the links with the string (hopefully a token when tokenizing) `[URL]`\n",
    "  message = re.sub(r'\\b(?:https?://|ftp://|www\\.)\\S+', '[URL]', message)\n",
    "  list_messages.append((message))\n",
    "\n",
    "  message_tokens_nltk = nltk_tokenizer(message)\n",
    "\n",
    "  messages_tokens.append(message_tokens_nltk) # array of arrays of string tokens\n",
    "\n",
    "messages_frame = pd.DataFrame(list_messages, columns=[\"content\"]) # why are we doing this? what do we need it for?\n",
    "# ok we need it for the below, can we not use anything else? like messages_dataset[\"content\"]\n",
    "all_messages_tokenized_with_apply = messages_frame.content.apply(nltk_tokenizer)\n",
    "\n",
    "\n",
    "#\n",
    "print(type(all_messages_tokenized_with_apply))\n",
    "print(type(messages_tokens))\n",
    "\n",
    "print(len(all_messages_tokenized_with_apply), len(messages_tokens))\n",
    "\n",
    "# check that the dataframe.apply tokenization works the same as the one done through iteration\n",
    "for i in range(len(messages_tokens)):\n",
    "  #print(messages_tokens[i])\n",
    "  #print(all_messages_tokenized_with_apply.iloc[i])\n",
    "  len_messages_tokens = len(messages_tokens[i])\n",
    "  len_all_tokenized_with_apply = len(all_messages_tokenized_with_apply.iloc[i])\n",
    "  if len_messages_tokens != len_all_tokenized_with_apply:\n",
    "    print(\"mismatch\", i, len(messages_tokens[i]), len(all_messages_tokenized_with_apply.iloc[i]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a vocabulary using the tokens created previously. Vocabulary is just the unique presence of the tokens into a python dictionary.\n",
    "\n",
    "We already have the vocebulary when creating the BPE tokens but for the other tokenizations mechanism we need to actually create the vocab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# A single variable with the (flattened) tokens from all messages\n",
    "# 14363 tokens in total in all messages??? how many unique ones??\n",
    "flat_tokens_with_apply = list(itertools.chain.from_iterable(all_messages_tokenized_with_apply))\n",
    "print(flat_tokens_with_apply)\n",
    "print(len(flat_tokens_with_apply))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "messages_counter_with_apply = Counter(flat_tokens_with_apply)\n",
    "\n",
    "print(f\"these are the unique tokens in the vocabulary? {len(messages_counter_with_apply)}\")\n",
    "print(messages_counter_with_apply)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think we can remove these\n",
    "messages_counter_with_apply.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "V = defaultdict(int)\n",
    "# let's create the vocabulary\n",
    "for tokens in messages_tokens:\n",
    "  for token in tokens:\n",
    "    V[token] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(V)\n",
    "print(f\"number of tokens = {len(V)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the Counter have a different number of elements from the `V` vocabulary??\n",
    "\n",
    "What do each of these represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = sorted(V.items(), key=lambda item: item[1], reverse=True)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len messages_counter_with_apply {len(messages_counter_with_apply)}\")\n",
    "print(f\"len(V)={len(V)}\")\n",
    "print(f\"len(messages_tokens) - len(V)={len(messages_tokens) - len(V)}\")\n",
    "# what are these 25 diff?\n",
    "\n",
    "\n",
    "messages_counter_list = .most_common(len())\n",
    "\n",
    "# print(type(V)) # V is a list of tuples\n",
    "\n",
    "not_in_V = []\n",
    "for token, frequency in messages_counter.items():\n",
    "  if (token, frequency) not in V:\n",
    "    #print(f\"this pair is not in V: {token, frequency}\")\n",
    "    not_in_V.append((token, frequency))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "So messages_counter has links that V doesn't have:\n",
    "not_in_V\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(not_in_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_big_text = \"\".join(texts)\n",
    "print(one_big_text)\n",
    "print(f\"these many chars in the messages corpus: {len(one_big_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the messages text into one big corpus. Let's use the BPE tokenization and see what tokens do we get.\n",
    "\n",
    "Then use the openai tiktoken and get the tokens from them and eventually compare all the tokenization mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's analyze the texts, let's see how many tokens do we have in total.\n",
    "\n",
    "Let's use difference type of tokenizers to get the tokens and compare which one is a better fit for our task.\n",
    "\n",
    "Let's also use the Porter Stemmer to stem our text, and also lowercase everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO what is the percentage of the blocked messages and not blocked messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a pandas Series type\n",
    "type(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_counts = messages.value_counts()\n",
    "print(messages_counts.head())\n",
    "messages_counts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Task\n",
    "\n",
    "- Print out the 50 most frequent (common) tokens in the reddit collection with their term frequencies (TF). \n",
    "  \n",
    "Use the python [collections.Counter](https://docs.python.org/2/library/collections.html) library. See it's documentation for examples on how to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(flat_tokens).most_common(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Text Normalization\n",
    "In this section we will apply simple text normalization. We will write a function that takes raw tokens and normalizes them.\n",
    "\n",
    "Define a python function called `normalize` that:\n",
    "- Takes a sequence of *tokens* as input \n",
    "- Returns a list of *normalized tokens*\n",
    "- The function should perform the following normalization: lowercasing (basic String operation) and stem the tokens using the PorterStemmer (see also the [NLTK stem package](https://www.nltk.org/api/nltk.stem.html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize(tokens):\n",
    "  normalized_tokens = [stemmer.stem(token.lower()) for token in tokens]\n",
    "  return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply the normalize function to the flat tokens. \n",
    "This may take a 1-2 minutes to run over the entire collection (it is over almost 5 million tokens)\n",
    "\n",
    "TODO update the above to reflect the actual tokens in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO we need to create the flat tokens\n",
    "normalized_tokens = normalize(flat_tokens)\n",
    "print(flat_tokens[:200])\n",
    "print(normalized_tokens[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect information on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of unique tokens (from flat_tokens)\n",
    "B = set(flat_tokens)\n",
    "print(f\"set of unique tokens (from flat_tokens) {B}\")\n",
    "\n",
    "# Set of unique normalized tokens --> the vocabulary\n",
    "V = \n",
    "\n",
    "# |N| - number of all tokens\n",
    "print(N)\n",
    "\n",
    "# |B|\n",
    "print(len(B))\n",
    "\n",
    "# |V| \n",
    "print(len(V)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Stopwords **\n",
    "\n",
    "The most common words are functional words, often referred to as 'stop words' because they don't convey meaningful information for 'aboutness' as discussed in the lecture. In many applications we remove stopwords to remove 'noise', but in other cases they may be important to keep. You should be able to justify your decisions for when (and what) words are 'stop words'.\n",
    "\n",
    "e.g. in language id, stopwords are important because stop words are not shared in common across languages, usually. We can therefore identify text based on it's expected usage patterns of these words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO this shouldn't be here\n",
    "exercise to the reader:\n",
    "- implement the treeabank word tokenizer using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora Datasheet\n",
    "\n",
    "Just for completeness of lecture 1. We'll explain the Linkedin messages corpus with its corpora datasheet.\n",
    "\n",
    "- motivation for collecting the corpus\n",
    "- situation: when and in what situation was text written/spoken\n",
    "- language variety: what language was the corpus in?\n",
    "- speaker demographics: what was e.g the age, sex of the text's authors?\n",
    "- collection process: how big is the data? if it is a subsample how was it sampled? was the data collected with consent? how was the data preprocessed? and what metadata is available?\n",
    "- annotation process: what are the annotations, how was the data annotated? how was the annotation process?\n",
    "- distribution: are there copyright or other intellectual property restrictions?\n",
    "\n",
    "here's the answers\n",
    "\n",
    "- motivation: just for fun, turned into something more serious\n",
    "- situation: messages received by people on linkedin\n",
    "- language variety: english\n",
    "- speaker demographic: different age, both sexes\n",
    "- collection process: received on linkedin, copied and pasted into a json file and put into a csv by the script in `scripts/process_linkedin_messages.py`\n",
    "- annotation process: annotation was manual and judged by me\n",
    "- distribution: no copyright\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- List things covered in the lecture but not done in the lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
