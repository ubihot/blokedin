{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9305358a",
   "metadata": {},
   "source": [
    "# Lecture Four: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46d9da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "we introduce an algorithm which is suited for discovering the link between features or clues and some particular outcome: **logistic regression**.\n",
    "\n",
    "In NLP, logistic regression is the based supervised learning algorithm for classification and it has a close relationship with neural networks.\n",
    "\n",
    "Logistic regression can be used to classify observation into one of two or more classes. In case of more than two classes that's called **multinomial logistic regression**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115e139",
   "metadata": {},
   "source": [
    "##### Generative vs Discriminative classifiers\n",
    "\n",
    "The difference between naive bayes and logistic regression is that logistic regression is **discriminative**.\n",
    "\n",
    "A disriminative model is only trying to learn to distinguish the classes (without learning much about them). e.g. if all the dogs were wearing collars and the cats were not, then that feature is enough for the model to distinguish them and if you ask it what it knows about cats all it can say that they don't wear collars.\n",
    "\n",
    "\n",
    "Like naive bayes also the logistic regression is a probabilistic classifier that makes use of supervised machine learining.\n",
    "\n",
    "a machine learing classification then has four components:\n",
    "- a **feature represenatation** of the input. for each input observation it'll have a vector of features [x1, x2, x3, xn]\n",
    "- a classification function that computes yhat, the estimated class, via p(y|x). **sigmoid** and **softmax** for classification\n",
    "- objective function that we want to optimize for learning: usually corresponding to minimizing the loss function on the error corresponding on the training examples. **cross-entropy loss function**\n",
    "- an algorithm for optimizing the objective function. **stochastic gradient descent**\n",
    "\n",
    "logistic regression has two phases:\n",
    "- **training**: we train the system (weights and bias) using stochastic gradient descrent and the cross entropy loss.\n",
    "- **test**: given a test example *x* we compute the p(y|x) and return the higher probability label y = 1 or y = 0.\n",
    "\n",
    "the goal of a binary logistic regression is to be able to make a binary decision about a class on a new input observation. The **sigmoid classifier** will help us in this.\n",
    "\n",
    "Logistic regression solves this task by learning from the training set a vector of **weights** and **bias term**. each weight wi is a real number, and is associated with one of the input feateures *xi*. It tells how much is the weight of that feature in the classification decision.\n",
    "\n",
    "The **bias term** is also called **intercept**, another real number.\n",
    "\n",
    "the **dot product** is used to calculate the decision of an input x and classify it using the features of the input x and the weights and adding the bias term.\n",
    "\n",
    "so we have:\n",
    "\n",
    "z = w dot x + b\n",
    "\n",
    "to note that in the equation above, nothing forces z to be a probabilitiy (i.e. a value between 0 and 1). To create a probablity we pass z throgh the **sigmoid** function Ïƒ(z). The sigmoid function (shaped like an s) is called the **logistic function** and gives logistic regression its name.\n",
    "\n",
    "the input to the sigmoid function, the score `z = w dot x + b` is called the **logit** because the logit function is the inverse of the sigmoid.\n",
    "\n",
    "The logit function is the log of the odds ratio `p / (1-p)`.\n",
    "\n",
    "```\n",
    "logit(p) = sigmainverse(p) = ln(p/(1-p))\n",
    "```\n",
    "\n",
    "by using the **logit** for z is to remind that using the sigmoid to turn z into a probability, we are implicitly interpreting z as not just any real number value but specifically a log odds.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f238e",
   "metadata": {},
   "source": [
    "### Classification with Logistic Regression\n",
    "\n",
    "the sigmoid function gives us a probability of an input x to belong to a class c, how do we make a decision about which class to apply to a test instance x?\n",
    "\n",
    "we need a **decision boundary**.\n",
    "\n",
    "\n",
    "cannot design feature by hands as that's exhaustive instead use **feature learining**, learn features in automatically in unsupervised way from input.\n",
    "\n",
    "when input features have large different range of values we need to scale rescale them so they have comparable ranges. We **standardize** the input values by centering them to result in zero mean and a standard deviation of 1 (this transformation is called the **z-score**) or we can **normalize** the input features values to lie between 0 and 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
