{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Three: Naive Bayes, Text Classification, and Sentiment\n",
    "TODO look at the TAD lectures when doing this lecture\n",
    "\n",
    "**Classification** lies at the heart of both human and machine intelligence.\n",
    "\n",
    "Many language processing tasks involve classification. In this lecture we'll introduce the naive Bayes algorithm for **text categorization**, the task of assigning a label or category to an entire text or document.\n",
    "\n",
    "Focus would be on the task of **sentiment analysis**, extraction of **sentiment**, positive or negative orientation that a writer expresses toward some object. e.g. a review of a movie, product or political tweet.\n",
    "\n",
    "The simplest version of sentiment analysis is binary classification task, and the words of the review provide excellent cues. e.g. words like *great*, *richly*, *awesome*, *pathetic*, *awful* are very informative.\n",
    "\n",
    "```\n",
    "+ ...characters and richly applied satire, and some great plot twists\n",
    "- It was pathetic. The worst part about it was the boxing scenes...\n",
    "```\n",
    "\n",
    "**Spam detection** is another important commercial application. Assigning a *spam* or *not-spam* to an email. You could use cues like \"WITHOUT ANY COST\" or \"Dear Winner\". It's similar to our blokedin goal.\n",
    "\n",
    "\n",
    "Another thing we might want to know about the language is in which language it's written in. The task of **language id** is the first step in most language processing pipelines. **Authorship attribution** determines the text's author.\n",
    "\n",
    "A part-of-speech tagger classifies each occurence of a word, e.g. verb, noun, etc..\n",
    "\n",
    "The goal of classification is to observe a single observation, extract some useful information and **classify** the observation into one of a discrete set of classes. You could use hand written rules by human but those are fragile. Instead use **supervised machine learning** where we have a data set of input observations, each associated with some correct output. The goal of the algorithm is to learn how to map from a new observation to a correct output.\n",
    "\n",
    "Formally, the task of classification is to take an input *x* and a fixed set of output classes *Y = {y1, y2, y3, yM}* and return predicted class *y belongs to Y*. \n",
    "\n",
    "\n",
    "A **probabilistic classifier** additionally will tell us the probability of the observation being in the class.\n",
    "\n",
    "**Generative Classifiers** like Naive Bayes build a model of how class could generate some input data; given an observation they return the class most likely to have generated the observation.\n",
    "\n",
    "**Discriminative Classifiers** like logistic regression instead learn what features from the input are most useful to discriminitate between the different possible classes.\n",
    "\n",
    "\n",
    "##### Naive Bayes Classifier\n",
    "We introduce the **multinomial naive Bayes classifier**, it's a bayesian classifier that makes a simplifying (naive) assumption about how features interact.\n",
    "\n",
    "We represent a document text as if it were a **bag of words**, i.e. an unordered set of words with their position ignored, keeping only their frequency in the document.\n",
    "\n",
    "Naive Bayes is a probabilistic classifier, meaning that for a document *d*, out of all classes *c belong to C* the classifier returns class *c^* which has the maximum posterior probability given the document. *^* to mean \"our correct estimate of the correct class\", and we use **argmax** to mean an operation that selects the argument (in this case the class *c*) that maximizes a function (in this case the probability P(c|d)).\n",
    "\n",
    "![alt text](../images/naive_argmax.png)\n",
    "\n",
    "This idea's **Bayesian inference**. The intuition of Bayesian inference is to use Bayes' rule to transform equation 4.1 into other probabilities that have some useful properties.\n",
    "\n",
    "![alt text](../images/bayes_rule.png)\n",
    "\n",
    "\n",
    "We call Naive Bayes a **generative** model cause we can read eq.4 as stating a kind of implicit assumption about how a document is generated; first a class is sampled from P(c) and then the words are generated by sampling P(d|c).\n",
    "\n",
    "\n",
    "We compute the most probable class *c^* given some document *d* choosing the class which has the highest product of two probabilities: the **prior probability** of the class P(c) and the **likelihood** of the document *P(d|c)*:\n",
    "\n",
    "![alt text](../images/prior_likelihood.png)\n",
    "\n",
    "without loss of generality we can represent a document *d* as a set of features *f1*, *f2*, *fn*:\n",
    "\n",
    "![alt text](../images/prior_likelihood_features.png)\n",
    "\n",
    "However, this is still complex to compute cause of the estimation of all the possible proabilities of the combination of the features, e.g. every possible set of words and position. Hence, Naive Bayes make two assumptions:\n",
    "- bag of words, assuming the features *f1*, *f2*, *fn* encode the word identity and not the position\n",
    "- **Naive Bayes assumption**: conditional independence assumption that the probabilities *P(fi|c)* are independent given the class *c* and hence can be 'naively' multiplied as follows:\n",
    "\n",
    "![text alt](../images/naive_bayes_assumption.png)\n",
    "\n",
    "\n",
    "By considering the features in log space and computing the predicted class as a linear function of input features, classifier that use linear combinations of the inputs to make classification decision like naive bayes and logistic regression are called **linear classifiers**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's implement the Naive Bayes Classifier\n",
    "\n",
    "Let's first create the **bag of words** from the [IMDB dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). We'll redo all these lectures in the end with the messages corpus just for fun, for education instead stick to something more serious.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset_filepath = \"../datasets/IMDB.csv\"\n",
    "messages_dataset_filepath = \"../datasets/train.csv\"\n",
    "\n",
    "#imdb_dataset = pd.read_csv(imdb_dataset_filepath)\n",
    "imdb_dataset = pd.read_csv(messages_dataset_filepath)\n",
    "\n",
    "print(imdb_dataset)\n",
    "#print(f\"total examples={len(imdb_dataset)}\")\n",
    "#print(f\"positive examples={len(imdb_dataset[imdb_dataset['sentiment'] == 'positive'])}\")\n",
    "#print(f\"negative examples={len(imdb_dataset[imdb_dataset['sentiment'] == 'negative'])}\")\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P^(c) = Nc/ Ndoc*\n",
    "so in our case for `positive` it's = 25000 / 50000 = 1/2. Same for `negative`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the bag of words from the imdb corpus\n",
    "def get_bag_of_words(dataset):\n",
    "  bag_of_words = set()\n",
    "  for row in dataset.iterrows():\n",
    "    review = row[1]\n",
    "    review = review.str.split(\" \")\n",
    "    review = review.to_list()[0]\n",
    "    for word in review:\n",
    "      bag_of_words.add(word)\n",
    "  #print(review)\n",
    "  return bag_of_words\n",
    "\n",
    "#print(bag_of_words)\n",
    "#print(len(bag_of_words))\n",
    "#for word in bag_of_words:\n",
    "#  print(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P(wi|c)* is the fraction of times the word *wi* appears among all words in all documents of topic c.\n",
    "\n",
    "We first concatenate all documents with category c into one big “category c” text.\n",
    "\n",
    "Then we use the frequency of *wi* in this concatenated document to give a maximum likelihood estimate of probability:\n",
    "\n",
    "![alt text](../images/mle_concatenated_documents.png)\n",
    "\n",
    "\n",
    "Here the vocabulary *V* consists of all the word types in all classes, not just the words in class *c*.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_of_topic_positive = imdb_dataset[imdb_dataset[\"block\"] == 0]\n",
    "documents_of_topic_negative = imdb_dataset[imdb_dataset[\"block\"] == 1]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents_of_topic_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents_of_topic_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_in_positive_documents = get_bag_of_words(documents_of_topic_positive)\n",
    "bag_of_words_in_positive_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_in_negative_documents = get_bag_of_words(documents_of_topic_negative)\n",
    "bag_of_words_in_negative_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue with MLE training. Imagine we try to estimate the likelihood of the word \"fantastic\" given the class \"positive\", but no training documents that contain the word \"fantastic\" and is classified as \"positive\". Perhaps \"fantastic\" used in a *sarcastic* way in *negative* class. In this case the probability would be 0.\n",
    "\n",
    "![alt text](../images/sarcastic_probability.png)\n",
    "\n",
    "Since naive Bayes multiplies all the feature likelihoods together, zero probabilities in the likelihood term for any class will cause the probability of the class to be zero, no matter the other evidence!\n",
    "\n",
    "The simples solution is to add the Laplace add-one smoothing, this one is commonly used in naive Bayes rather than language models.\n",
    "\n",
    "![alt text](../images/naive_bayes_add_laplace.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V is our vocabulary\n",
    "V = bag_of_words_in_positive_documents.union(bag_of_words_in_negative_documents)\n",
    "V_sure = get_bag_of_words(imdb_dataset)\n",
    "assert V == V_sure, f\"expected {len(V_sure)} but got {len(V)}\"\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we do about **unknown** words? i.e. words that did not occur in the training document of any class and are not in our vocabulary but did appear in the test data.\n",
    "\n",
    "The solution for this is to ignore them. Remove them from the test document and not include any probability for them at all. TODO why is this the case???\n",
    "\n",
    "\n",
    "Some systems may choose to ignore another class of words: **stop words**, very frequent words like *the* and *a*. Sort the vocabulary and remove the top 10-100 vocabulary entries as stop words or use already predefined online. Then each istance of these is removed from both test and training documents as if it never occurred.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worked example\n",
    "let's calculate the *P(c)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_examples = len(imdb_dataset)\n",
    "positive_example_counts = len(imdb_dataset[imdb_dataset[\"sentiment\"] == \"positive\"])\n",
    "negative_example_counts = len(imdb_dataset[imdb_dataset[\"sentiment\"] == \"negative\"])\n",
    "prior_positive_class = positive_example_counts / total_examples\n",
    "prior_negative_class = negative_example_counts / total_examples\n",
    "\n",
    "print(f\"prior_positive_class={prior_positive_class}\")\n",
    "print(f\"prior_negative_class={prior_negative_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../images/prior_likelihood_features.png)\n",
    "\n",
    "to apply Naive Bayes classifier to text, we will use each word in the documents as a feature and we consider each of the words in the document by walking an index through every word position in the document.\n",
    "\n",
    "![alt text](../images/position_words.png)\n",
    "    \n",
    "\n",
    "Naive Bayes calculations are done in log space for same reason explained earlier.\n",
    "\n",
    "![alt text](../images/naive_bayes_log.png)\n",
    "\n",
    "#### Training the Naive Bayes Classifier\n",
    "How can we learn the probabilities of *P(c)* i.e. the *prior* and *P(fi|c)* i.e. *likelihood*?\n",
    "\n",
    "First we'll consider the MLE. Simply use the frequencies in the data. For the class prior *P(c)* we ask which percentage of the documents in our training data class *c* and the *Ndoc* be the total number of documents. Then:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# TODO try this at some point potentially filter stop words\n",
    "#from sklearn.feature_extraction import text\n",
    "#stop_words = set(text.ENGLISH_STOP_WORDS)\n",
    "def get_bag_of_words(dataset):\n",
    "  return set(\n",
    "    dataset[\"text\"] # access all the text column\n",
    "    .dropna()       # remove missing values\n",
    "    #.str.lower()\n",
    "    .str.split()    # split on whitespaces (more efficient than \" \")\n",
    "    .explode()      # flatten all words into one Series\n",
    "    #.loc(lambda x: ~x.isin(stop_words)) # filter stopwords early\n",
    "  )\n",
    "\n",
    "\n",
    "# we're keeping the punctuation and everything and just considering the\n",
    "# words as split by whitespaces so some of these results may not be accurate\n",
    "def train_naive_bayes():\n",
    "  total_examples = len(imdb_dataset)\n",
    "  positive_example_counts = len(imdb_dataset[imdb_dataset[\"block\"] == 0])\n",
    "  negative_example_counts = len(imdb_dataset[imdb_dataset[\"block\"] == 1])\n",
    "\n",
    "  prior_positive_class = np.log2(positive_example_counts/total_examples)\n",
    "  prior_negative_class = np.log2(negative_example_counts/total_examples)\n",
    "\n",
    "  print(prior_positive_class, prior_negative_class)\n",
    "\n",
    "  # compute the vocabulary V\n",
    "  bag_of_words_in_negative_documents = get_bag_of_words(documents_of_topic_negative)\n",
    "  bag_of_words_in_positive_documents = get_bag_of_words(documents_of_topic_positive)\n",
    "  V = bag_of_words_in_positive_documents.union(bag_of_words_in_negative_documents)\n",
    "  V_sure = get_bag_of_words(imdb_dataset)\n",
    "  assert V == V_sure, f\"expected {len(V_sure)} but got {len(V)}\"\n",
    "\n",
    "  bigdoc = {\n",
    "    \"positive\": \" \".join(documents_of_topic_positive[\"text\"].astype(str)),\n",
    "    \"negative\": \" \".join(documents_of_topic_negative[\"text\"].astype(str)),\n",
    "  }\n",
    "\n",
    "  # precompile a regex pattern to match all words in V\n",
    "  sorted_words = sorted(V, key=lambda x: (-len(x), x))\n",
    "  pattern_str = r'\\b' + '|'.join([re.escape(word) for word in sorted_words]) + r'\\b'\n",
    "  word_regex = re.compile(pattern_str)\n",
    "\n",
    "  def count_words(text): return Counter(word_regex.findall(text))\n",
    "\n",
    "  # do we really need to do it this way???\n",
    "  # I think we should remove the first 21 entries from these\n",
    "  positive_counts = count_words(bigdoc[\"positive\"])\n",
    "  negative_counts = count_words(bigdoc[\"negative\"])\n",
    "\n",
    "  # build counts dictionary\n",
    "  counts = {}\n",
    "  for word in V:\n",
    "    counts[(word, \"positive\")] = positive_counts.get(word, 0)\n",
    "    counts[(word, \"negative\")] = negative_counts.get(word, 0)\n",
    "\n",
    "  return V, counts, positive_counts, negative_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.599037685932879 -0.2602357724811205\n"
     ]
    }
   ],
   "source": [
    "\n",
    "V, counts, positive_counts, negative_counts = train_naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(V)\n",
    "print(len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts)\n",
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'to': 29,\n",
       "         'the': 27,\n",
       "         'a': 25,\n",
       "         'and': 22,\n",
       "         'for': 18,\n",
       "         'you': 17,\n",
       "         'is': 17,\n",
       "         'of': 16,\n",
       "         'in': 12,\n",
       "         'with': 12,\n",
       "         'I': 10,\n",
       "         'your': 10,\n",
       "         'are': 10,\n",
       "         'be': 10,\n",
       "         'at': 8,\n",
       "         'Hi': 7,\n",
       "         'on': 7,\n",
       "         'have': 6,\n",
       "         'team': 5,\n",
       "         'an': 5,\n",
       "         'thought': 4,\n",
       "         'can': 4,\n",
       "         'work': 4,\n",
       "         'The': 4,\n",
       "         'Hut': 4,\n",
       "         'currently': 4,\n",
       "         'our': 4,\n",
       "         'if': 4,\n",
       "         'or': 4,\n",
       "         'London': 4,\n",
       "         'role': 4,\n",
       "         'we': 3,\n",
       "         'from': 3,\n",
       "         'connecting': 3,\n",
       "         'this': 3,\n",
       "         'as': 3,\n",
       "         'global': 3,\n",
       "         'technology': 3,\n",
       "         'graduates': 3,\n",
       "         'me': 3,\n",
       "         'know': 3,\n",
       "         'up': 3,\n",
       "         'like': 3,\n",
       "         'more': 3,\n",
       "         'interested': 3,\n",
       "         'experience': 3,\n",
       "         'will': 3,\n",
       "         'am': 3,\n",
       "         'great': 3,\n",
       "         'doing': 3,\n",
       "         'how': 3,\n",
       "         'it': 3,\n",
       "         'us': 3,\n",
       "         'profile': 2,\n",
       "         'both': 2,\n",
       "         'here': 2,\n",
       "         'what': 2,\n",
       "         'Ubaidullah,': 2,\n",
       "         'part': 2,\n",
       "         'Group': 2,\n",
       "         '&': 2,\n",
       "         'leading': 2,\n",
       "         'online': 2,\n",
       "         'world-class': 2,\n",
       "         'business,': 2,\n",
       "         'I’m': 2,\n",
       "         'funds': 2,\n",
       "         'ambitious': 2,\n",
       "         'that': 2,\n",
       "         'Technology': 2,\n",
       "         'THG': 2,\n",
       "         'out': 2,\n",
       "         'day': 2,\n",
       "         'interview': 2,\n",
       "         'hedge': 2,\n",
       "         'week': 2,\n",
       "         'Hey': 2,\n",
       "         'company': 2,\n",
       "         'engineer': 2,\n",
       "         'new': 2,\n",
       "         'Consultant': 2,\n",
       "         'all': 2,\n",
       "         'going': 2,\n",
       "         'hiring': 2,\n",
       "         'Aspect': 2,\n",
       "         'systems': 2,\n",
       "         'fit': 2,\n",
       "         'let': 2,\n",
       "         'time': 2,\n",
       "         'well.': 2,\n",
       "         'been': 2,\n",
       "         'see': 2,\n",
       "         'job': 2,\n",
       "         'after': 2,\n",
       "         'Backend': 2,\n",
       "         'Engineering': 2,\n",
       "         'please': 2,\n",
       "         'whether': 2,\n",
       "         'Ubaidullah\\\\n\\\\n': 1,\n",
       "         'noticed': 1,\n",
       "         'benefit': 1,\n",
       "         'linkedin.\\\\n': 1,\n",
       "         'sincerely\\\\n\\\\n': 1,\n",
       "         'Omar': 1,\n",
       "         'Adam': 1,\n",
       "         'Ciao': 1,\n",
       "         'Ubi!': 1,\n",
       "         \"what's\": 1,\n",
       "         'bro': 1,\n",
       "         'courses': 1,\n",
       "         'taking': 1,\n",
       "         'year?': 1,\n",
       "         '\\\\n\\\\nMy': 1,\n",
       "         'name': 1,\n",
       "         'Steve': 1,\n",
       "         'recruitment': 1,\n",
       "         'Group.\\\\n\\\\nThe': 1,\n",
       "         'aims': 1,\n",
       "         'World’s': 1,\n",
       "         'Leading': 1,\n",
       "         'Online': 1,\n",
       "         'Health': 1,\n",
       "         'Beauty': 1,\n",
       "         'Destination.': 1,\n",
       "         'Headquartered': 1,\n",
       "         'Manchester,': 1,\n",
       "         'UK’s': 1,\n",
       "         'multi-website': 1,\n",
       "         'retailer': 1,\n",
       "         'rapidly': 1,\n",
       "         'growing': 1,\n",
       "         'operations': 1,\n",
       "         '(including': 1,\n",
       "         'acquisitions': 1,\n",
       "         'IPO)': 1,\n",
       "         'proprietary': 1,\n",
       "         'platform,': 1,\n",
       "         'disruptive': 1,\n",
       "         'business': 1,\n",
       "         'model,': 1,\n",
       "         'ambition': 1,\n",
       "         'digital': 1,\n",
       "         'leader.\\\\n\\\\nThe': 1,\n",
       "         'reason': 1,\n",
       "         'contact': 1,\n",
       "         'personally': 1,\n",
       "         'building': 1,\n",
       "         'high-level': 1,\n",
       "         '2020': 1,\n",
       "         'graduate': 1,\n",
       "         \"Group's\": 1,\n",
       "         'team.': 1,\n",
       "         'We': 1,\n",
       "         'strong': 1,\n",
       "         'mix': 1,\n",
       "         'Software': 1,\n",
       "         'Engineers,': 1,\n",
       "         'Data': 1,\n",
       "         'Scientists.Engineers,': 1,\n",
       "         'DevOps/Infrastructure': 1,\n",
       "         'Network': 1,\n",
       "         'Engineers\\\\n\\\\nThis': 1,\n",
       "         'led': 1,\n",
       "         'by': 1,\n",
       "         'technologists': 1,\n",
       "         'diverse': 1,\n",
       "         'range': 1,\n",
       "         'backgrounds': 1,\n",
       "         'Silicon': 1,\n",
       "         'Valley': 1,\n",
       "         'tech': 1,\n",
       "         'giants': 1,\n",
       "         '(Google,': 1,\n",
       "         'Amazon)': 1,\n",
       "         'through': 1,\n",
       "         'Hedge': 1,\n",
       "         'Algorithmic': 1,\n",
       "         'Trading': 1,\n",
       "         'Firms\\\\n\\\\nThe': 1,\n",
       "         '#1': 1,\n",
       "         'place': 1,\n",
       "         'want': 1,\n",
       "         'develop': 1,\n",
       "         'leadership': 1,\n",
       "         'skills': 1,\n",
       "         'forefront': 1,\n",
       "         'e-commerce.': 1,\n",
       "         'Today,': 1,\n",
       "         'we’ve': 1,\n",
       "         'got': 1,\n",
       "         '27-year': 1,\n",
       "         'old': 1,\n",
       "         'running': 1,\n",
       "         '£80m+': 1,\n",
       "         'consumer': 1,\n",
       "         'division,': 1,\n",
       "         '28-year-old': 1,\n",
       "         'Chief': 1,\n",
       "         'Officer': 1,\n",
       "         'managing': 1,\n",
       "         '200': 1,\n",
       "         'revenue': 1,\n",
       "         '£500m+.': 1,\n",
       "         'Both': 1,\n",
       "         'joined': 1,\n",
       "         'straight': 1,\n",
       "         'University': 1,\n",
       "         'no': 1,\n",
       "         'prior': 1,\n",
       "         'experience.\\\\n\\\\nWe': 1,\n",
       "         'hire': 1,\n",
       "         'best': 1,\n",
       "         'provide': 1,\n",
       "         'them': 1,\n",
       "         'opportunity': 1,\n",
       "         'climb': 1,\n",
       "         'ladder': 1,\n",
       "         'quickly,': 1,\n",
       "         'offering': 1,\n",
       "         'real': 1,\n",
       "         'responsibility': 1,\n",
       "         'one.\\\\n\\\\nOur': 1,\n",
       "         'process': 1,\n",
       "         'technical': 1,\n",
       "         'exercise': 1,\n",
       "         'take': 1,\n",
       "         'remotely': 1,\n",
       "         'then': 1,\n",
       "         '1': 1,\n",
       "         'stage': 1,\n",
       "         'face': 1,\n",
       "         'face.': 1,\n",
       "         'So': 1,\n",
       "         'very': 1,\n",
       "         'smooth': 1,\n",
       "         'flexible': 1,\n",
       "         'times/dates': 1,\n",
       "         'accommodate': 1,\n",
       "         'studies.\\\\n\\\\nLet': 1,\n",
       "         'interested?\\\\n\\\\nThanks,\\\\nSteve\\\\n\\\\n': 1,\n",
       "         'Stephen': 1,\n",
       "         'Taylor\\\\nRecruitment': 1,\n",
       "         'Partner': 1,\n",
       "         '-': 1,\n",
       "         'Ubaidullah.': 1,\n",
       "         'recruit': 1,\n",
       "         'Computer': 1,\n",
       "         'Science': 1,\n",
       "         'into': 1,\n",
       "         'London’s': 1,\n",
       "         'start-ups,': 1,\n",
       "         'consultancies.': 1,\n",
       "         'recruiting': 1,\n",
       "         'three': 1,\n",
       "         'exceptional': 1,\n",
       "         'roles': 1,\n",
       "         'paying': 1,\n",
       "         '£65k.': 1,\n",
       "         'Would': 1,\n",
       "         'info': 1,\n",
       "         'keep': 1,\n",
       "         'touch': 1,\n",
       "         'future?': 1,\n",
       "         'Thanks': 1,\n",
       "         'endorsing': 1,\n",
       "         'Scrum!': 1,\n",
       "         'Congrats': 1,\n",
       "         'anniversary!': 1,\n",
       "         'Nice': 1,\n",
       "         'meet': 1,\n",
       "         'last': 1,\n",
       "         'Ubaidullah,\\\\n\\\\nThanks': 1,\n",
       "         'me.': 1,\n",
       "         'Ubaidullah,\\\\n\\\\nAre': 1,\n",
       "         'joining': 1,\n",
       "         'product': 1,\n",
       "         'wealth': 1,\n",
       "         'management': 1,\n",
       "         'space?\\\\n\\\\nA': 1,\n",
       "         'Head': 1,\n",
       "         'Applications': 1,\n",
       "         'Series': 1,\n",
       "         'C-funded': 1,\n",
       "         'b2b': 1,\n",
       "         'FinTech': 1,\n",
       "         'searching': 1,\n",
       "         'mid-level': 1,\n",
       "         '2+': 1,\n",
       "         'years': 1,\n",
       "         'industry': 1,\n",
       "         'join': 1,\n",
       "         'team.\\\\n\\\\nYou': 1,\n",
       "         'build': 1,\n",
       "         'features,': 1,\n",
       "         'enhance': 1,\n",
       "         'performance': 1,\n",
       "         'existing': 1,\n",
       "         'APIs,': 1,\n",
       "         'understand': 1,\n",
       "         'customer': 1,\n",
       "         'requirements': 1,\n",
       "         'drive': 1,\n",
       "         'innovation': 1,\n",
       "         'across': 1,\n",
       "         'products.\\\\n\\\\nThe': 1,\n",
       "         'impressive': 1,\n",
       "         'list': 1,\n",
       "         'customers;': 1,\n",
       "         'HSBC,': 1,\n",
       "         'Hulius': 1,\n",
       "         'Bar,': 1,\n",
       "         'Fidelity,': 1,\n",
       "         'DBS,': 1,\n",
       "         'more...\\\\n\\\\nAs': 1,\n",
       "         'Node.js': 1,\n",
       "         'FS': 1,\n",
       "         'sector,': 1,\n",
       "         'profile.\\\\n\\\\nUp': 1,\n",
       "         '£60,000': 1,\n",
       "         '+': 1,\n",
       "         'bonus\\\\n\\\\nOne': 1,\n",
       "         'office.\\\\n\\\\nWould': 1,\n",
       "         'info?\\\\n\\\\nTom': 1,\n",
       "         'Fern\\\\nCloud': 1,\n",
       "         'Orbis': 1,\n",
       "         'Consultants': 1,\n",
       "         'Ubaidullah,\\\\n\\\\nHope': 1,\n",
       "         'well.\\\\n\\\\nI': 1,\n",
       "         'keen': 1,\n",
       "         'connect': 1,\n",
       "         'one': 1,\n",
       "         'my': 1,\n",
       "         'clients': 1,\n",
       "         'Capital,': 1,\n",
       "         'Systematic': 1,\n",
       "         'fund': 1,\n",
       "         'offices': 1,\n",
       "         'US.\\\\nThe': 1,\n",
       "         'python': 1,\n",
       "         'quantitative': 1,\n",
       "         'developer': 1,\n",
       "         'variety': 1,\n",
       "         'alongside': 1,\n",
       "         'example,': 1,\n",
       "         'signal': 1,\n",
       "         'generation': 1,\n",
       "         'trade': 1,\n",
       "         'scheduling': 1,\n",
       "         'engine.\\\\n\\\\nI': 1,\n",
       "         'looking': 1,\n",
       "         'individual': 1,\n",
       "         'background': 1,\n",
       "         'seems': 1,\n",
       "         'Capital.': 1,\n",
       "         '\\\\n\\\\nWould': 1,\n",
       "         'discuss': 1,\n",
       "         'further,': 1,\n",
       "         'when': 1,\n",
       "         'connect?\\\\n\\\\nThanks,\\\\nBen\\\\n\\\\nBen': 1,\n",
       "         'Lam\\\\nRecruitment': 1,\n",
       "         'Radley': 1,\n",
       "         'James': 1,\n",
       "         'Ubaid,\\\\n\\\\nI': 1,\n",
       "         'hope': 1,\n",
       "         \"you're\": 1,\n",
       "         \"It's\": 1,\n",
       "         'while': 1,\n",
       "         'since': 1,\n",
       "         'Equals,': 1,\n",
       "         \"I'd\": 1,\n",
       "         'reach': 1,\n",
       "         'things': 1,\n",
       "         \"you.\\\\n\\\\nI've\": 1,\n",
       "         'recently': 1,\n",
       "         'started': 1,\n",
       "         'search': 1,\n",
       "         'myself': 1,\n",
       "         'graduating': 1,\n",
       "         \"you've\": 1,\n",
       "         'hunt': 1,\n",
       "         'was': 1,\n",
       "         'wondering': 1,\n",
       "         \"you'd\": 1,\n",
       "         'do': 1,\n",
       "         'some': 1,\n",
       "         'prep': 1,\n",
       "         'bounce': 1,\n",
       "         'ideas': 1,\n",
       "         'off': 1,\n",
       "         'each': 1,\n",
       "         'other?': 1,\n",
       "         'might': 1,\n",
       "         'helpful': 1,\n",
       "         'practice': 1,\n",
       "         'together': 1,\n",
       "         'share': 1,\n",
       "         'tips': 1,\n",
       "         'feedback.': 1,\n",
       "         'Plus': 1,\n",
       "         'would': 1,\n",
       "         'catch': 1,\n",
       "         'time\\\\n\\\\nNo': 1,\n",
       "         'pressure': 1,\n",
       "         'all,': 1,\n",
       "         \"it's\": 1,\n",
       "         'completely': 1,\n",
       "         'interested.': 1,\n",
       "         'Of': 1,\n",
       "         'course': 1,\n",
       "         'quite': 1,\n",
       "         'senior': 1,\n",
       "         'mine.': 1,\n",
       "         'Ubaid.': 1,\n",
       "         '\\\\n': 1,\n",
       "         'I’d': 1,\n",
       "         'check': 1,\n",
       "         'Ubaidullah,\\\\n\\\\nI': 1,\n",
       "         'reviewed': 1,\n",
       "         'think': 1,\n",
       "         'could': 1,\n",
       "         'potentially': 1,\n",
       "         'role.\\\\n\\\\nIf': 1,\n",
       "         'interested,': 1,\n",
       "         'complete': 1,\n",
       "         'following': 1,\n",
       "         'form': 1,\n",
       "         'start': 1,\n",
       "         'process.': 1,\n",
       "         'This': 1,\n",
       "         'gives': 1,\n",
       "         'good': 1,\n",
       "         'understanding': 1,\n",
       "         'makes': 1,\n",
       "         'sense': 1,\n",
       "         'move': 1,\n",
       "         'next': 1,\n",
       "         'step.': 1,\n",
       "         'https://forms.gle/oiaLpDGwDMvSjgKt8\\\\n\\\\nBear': 1,\n",
       "         'mind': 1,\n",
       "         'onsite': 1,\n",
       "         'Mon-Fri': 1,\n",
       "         'so': 1,\n",
       "         'based': 1,\n",
       "         'need': 1,\n",
       "         'relocation.\\\\n\\\\nFeel': 1,\n",
       "         'free': 1,\n",
       "         'message': 1,\n",
       "         'back': 1,\n",
       "         'any': 1,\n",
       "         'questions!\\\\n\\\\nBest,\\\\nEdoardo': 1,\n",
       "         'hello': 1,\n",
       "         'hello,': 1,\n",
       "         'mate?': 1,\n",
       "         'backend': 1,\n",
       "         'Ubaid!': 1,\n",
       "         'How': 1,\n",
       "         'you?': 1,\n",
       "         '\\\\n\\\\nI': 1,\n",
       "         'just': 1,\n",
       "         'tried': 1,\n",
       "         'call,': 1,\n",
       "         '\\\\n\\\\nYou': 1,\n",
       "         'applied': 1,\n",
       "         'LinkedIn': 1,\n",
       "         'Node.JS': 1,\n",
       "         'Developer': 1,\n",
       "         'position.': 1,\n",
       "         'description': 1,\n",
       "         'attached.\\\\n\\\\nWhat': 1,\n",
       "         'availability': 1,\n",
       "         'position': 1,\n",
       "         '(notice': 1,\n",
       "         'period),': 1,\n",
       "         'salary': 1,\n",
       "         'expectation,': 1,\n",
       "         'please?\\\\n\\\\nThis': 1,\n",
       "         'hybrid': 1,\n",
       "         'Islington': 1,\n",
       "         '(onsite': 1,\n",
       "         '2': 1,\n",
       "         'days': 1,\n",
       "         'week)': 1,\n",
       "         '£55-£60k': 1,\n",
       "         'p/a\\\\n\\\\nI': 1,\n",
       "         'look': 1,\n",
       "         'forward': 1,\n",
       "         'thoughts': 1,\n",
       "         'spec': 1,\n",
       "         'hopefully': 1,\n",
       "         'discussing': 1,\n",
       "         'further.\\\\n\\\\nBR': 1,\n",
       "         '\\\\nTom': 1,\n",
       "         'Pollock': 1,\n",
       "         '\\\\ntom.pollock@consortia.com\\\\nAssociate': 1,\n",
       "         'Director': 1,\n",
       "         '@': 1,\n",
       "         'Consortia': 1,\n",
       "         'Group\\\\n': 1,\n",
       "         '\\\\nConnecting': 1,\n",
       "         'top-tier': 1,\n",
       "         'AI': 1,\n",
       "         'Talent': 1})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'to': 347,\n",
       "         'a': 329,\n",
       "         'and': 310,\n",
       "         'the': 259,\n",
       "         'you': 185,\n",
       "         'in': 180,\n",
       "         'of': 172,\n",
       "         'for': 143,\n",
       "         'with': 120,\n",
       "         'are': 108,\n",
       "         'be': 98,\n",
       "         'is': 96,\n",
       "         'I': 78,\n",
       "         'at': 78,\n",
       "         'your': 77,\n",
       "         'on': 75,\n",
       "         'have': 65,\n",
       "         'that': 57,\n",
       "         '-': 55,\n",
       "         'this': 54,\n",
       "         'an': 54,\n",
       "         'Hi': 49,\n",
       "         'looking': 47,\n",
       "         'experience': 46,\n",
       "         'will': 42,\n",
       "         'role': 41,\n",
       "         'working': 41,\n",
       "         'would': 41,\n",
       "         'as': 40,\n",
       "         'our': 39,\n",
       "         'their': 38,\n",
       "         'team': 37,\n",
       "         'from': 36,\n",
       "         'if': 36,\n",
       "         'more': 33,\n",
       "         'can': 33,\n",
       "         'me': 32,\n",
       "         'about': 32,\n",
       "         'new': 32,\n",
       "         'or': 31,\n",
       "         'great': 31,\n",
       "         'they': 30,\n",
       "         'interested': 29,\n",
       "         'up': 29,\n",
       "         'currently': 29,\n",
       "         '|': 29,\n",
       "         'am': 28,\n",
       "         'Developer': 28,\n",
       "         'work': 27,\n",
       "         'know': 26,\n",
       "         '&': 26,\n",
       "         'we': 25,\n",
       "         'some': 25,\n",
       "         'so': 25,\n",
       "         'let': 24,\n",
       "         'who': 24,\n",
       "         'Python': 22,\n",
       "         'like': 22,\n",
       "         '/': 22,\n",
       "         'please': 21,\n",
       "         'across': 21,\n",
       "         'out': 21,\n",
       "         'profile': 21,\n",
       "         'Software': 21,\n",
       "         'company': 20,\n",
       "         'Ubaidullah,': 19,\n",
       "         'tech': 19,\n",
       "         'see': 18,\n",
       "         '\\\\n\\\\nI': 18,\n",
       "         'opportunity': 18,\n",
       "         'join': 18,\n",
       "         'within': 18,\n",
       "         'time': 18,\n",
       "         'They': 18,\n",
       "         'Ubaidullah,\\\\n\\\\nI': 18,\n",
       "         'London': 18,\n",
       "         'really': 17,\n",
       "         'Consultant': 17,\n",
       "         'platform': 17,\n",
       "         'could': 16,\n",
       "         'has': 16,\n",
       "         'by': 16,\n",
       "         'hope': 16,\n",
       "         'free': 16,\n",
       "         'it': 16,\n",
       "         'Backend': 16,\n",
       "         'building': 16,\n",
       "         'Hey': 16,\n",
       "         'Engineer': 16,\n",
       "         'forward': 15,\n",
       "         'client': 15,\n",
       "         'good': 15,\n",
       "         'discuss': 15,\n",
       "         'Recruitment': 15,\n",
       "         'development': 14,\n",
       "         'call': 14,\n",
       "         'fit': 14,\n",
       "         'based': 14,\n",
       "         'exciting': 14,\n",
       "         'one': 13,\n",
       "         'look': 13,\n",
       "         'Talent': 13,\n",
       "         'technology': 13,\n",
       "         'all': 13,\n",
       "         'leading': 13,\n",
       "         'Java': 13,\n",
       "         'open': 13,\n",
       "         '–': 13,\n",
       "         'AI': 13,\n",
       "         'do': 12,\n",
       "         'The': 12,\n",
       "         'best': 12,\n",
       "         'week': 12,\n",
       "         'how': 12,\n",
       "         'through': 12,\n",
       "         'my': 12,\n",
       "         'into': 12,\n",
       "         'just': 12,\n",
       "         'interest': 12,\n",
       "         \"you're\": 12,\n",
       "         'people': 12,\n",
       "         'AWS': 12,\n",
       "         'any': 11,\n",
       "         'days': 11,\n",
       "         'think': 11,\n",
       "         'systems': 11,\n",
       "         'thought': 11,\n",
       "         'opportunities': 11,\n",
       "         'hearing': 11,\n",
       "         'UK': 11,\n",
       "         'business': 11,\n",
       "         'growing': 11,\n",
       "         'backend': 11,\n",
       "         'next': 11,\n",
       "         'roles': 11,\n",
       "         'love': 11,\n",
       "         'top': 11,\n",
       "         'per': 10,\n",
       "         'want': 10,\n",
       "         'training': 10,\n",
       "         'part': 10,\n",
       "         'available': 10,\n",
       "         'help': 10,\n",
       "         'Engineering': 10,\n",
       "         'what': 10,\n",
       "         'reach': 10,\n",
       "         'skills': 10,\n",
       "         'came': 10,\n",
       "         'team,': 10,\n",
       "         'office': 10,\n",
       "         'remote': 10,\n",
       "         'but': 10,\n",
       "         'Tech': 10,\n",
       "         'Senior': 10,\n",
       "         '+': 10,\n",
       "         'over': 9,\n",
       "         'If': 9,\n",
       "         'Technology': 9,\n",
       "         'was': 9,\n",
       "         'keen': 9,\n",
       "         'software': 9,\n",
       "         'industry': 9,\n",
       "         'salary': 9,\n",
       "         'talent': 9,\n",
       "         'wanted': 9,\n",
       "         'companies': 9,\n",
       "         'engineers': 9,\n",
       "         'doing': 8,\n",
       "         'As': 8,\n",
       "         'most': 8,\n",
       "         'which': 8,\n",
       "         'learn': 8,\n",
       "         'innovative': 8,\n",
       "         'also': 8,\n",
       "         'design': 8,\n",
       "         'might': 8,\n",
       "         'CV': 8,\n",
       "         'when': 8,\n",
       "         'career': 8,\n",
       "         \"I'm\": 8,\n",
       "         'recruiting': 8,\n",
       "         'projects': 8,\n",
       "         'Specialist': 8,\n",
       "         'using': 8,\n",
       "         'hire': 8,\n",
       "         'not': 8,\n",
       "         'few': 8,\n",
       "         'looks': 8,\n",
       "         'there': 8,\n",
       "         'engineering': 8,\n",
       "         'FinTech': 8,\n",
       "         'CISSP': 7,\n",
       "         'provide': 7,\n",
       "         'Recruiter': 7,\n",
       "         'job': 7,\n",
       "         'here': 7,\n",
       "         'You': 7,\n",
       "         'get': 7,\n",
       "         'lot': 7,\n",
       "         'send': 7,\n",
       "         'strong': 7,\n",
       "         'market': 7,\n",
       "         'fully': 7,\n",
       "         'team.': 7,\n",
       "         'make': 7,\n",
       "         'engineer': 7,\n",
       "         'teams': 7,\n",
       "         'investment': 7,\n",
       "         'other': 7,\n",
       "         'them': 7,\n",
       "         '3': 7,\n",
       "         '\\\\n•': 7,\n",
       "         'Equals': 7,\n",
       "         'Data': 7,\n",
       "         'Headhunter': 7,\n",
       "         'Hunter': 7,\n",
       "         'Bond': 7,\n",
       "         '•': 6,\n",
       "         '2': 6,\n",
       "         'clients': 6,\n",
       "         'around': 6,\n",
       "         'offer': 6,\n",
       "         'knowledge': 6,\n",
       "         'where': 6,\n",
       "         'message': 6,\n",
       "         'go': 6,\n",
       "         'quick': 6,\n",
       "         'global': 6,\n",
       "         'process': 6,\n",
       "         'Developers': 6,\n",
       "         'having': 6,\n",
       "         \"you'd\": 6,\n",
       "         'candidate': 6,\n",
       "         'role,': 6,\n",
       "         'spec': 6,\n",
       "         'Group': 6,\n",
       "         'you’re': 6,\n",
       "         'highly': 6,\n",
       "         'Engineers': 6,\n",
       "         'today': 6,\n",
       "         '\\\\n\\\\nThe': 6,\n",
       "         \"I'd\": 6,\n",
       "         'reaching': 6,\n",
       "         'set': 6,\n",
       "         'need': 6,\n",
       "         'scale': 6,\n",
       "         'grow': 6,\n",
       "         'hear': 6,\n",
       "         'API': 6,\n",
       "         'services': 6,\n",
       "         'London,': 6,\n",
       "         'high': 6,\n",
       "         'Up': 6,\n",
       "         'build': 6,\n",
       "         'Money': 6,\n",
       "         'product': 6,\n",
       "         'led': 5,\n",
       "         'course': 5,\n",
       "         'getting': 5,\n",
       "         'position': 5,\n",
       "         'Are': 5,\n",
       "         'someone': 5,\n",
       "         'Acquisition': 5,\n",
       "         'Morgan': 5,\n",
       "         'secure': 5,\n",
       "         'code': 5,\n",
       "         'stack': 5,\n",
       "         'programming': 5,\n",
       "         '\\\\n\\\\nIf': 5,\n",
       "         'something': 5,\n",
       "         'use': 5,\n",
       "         'recent': 5,\n",
       "         'come': 5,\n",
       "         'share': 5,\n",
       "         'many': 5,\n",
       "         'number': 5,\n",
       "         'then': 5,\n",
       "         'large': 5,\n",
       "         'This': 5,\n",
       "         'chance': 5,\n",
       "         'Development': 5,\n",
       "         'saw': 5,\n",
       "         'End': 5,\n",
       "         'take': 5,\n",
       "         'project': 5,\n",
       "         'every': 5,\n",
       "         'day': 5,\n",
       "         'culture': 5,\n",
       "         'firm': 5,\n",
       "         'above': 5,\n",
       "         'Salesforce': 5,\n",
       "         'Java,': 5,\n",
       "         'first': 5,\n",
       "         'further': 5,\n",
       "         'works': 5,\n",
       "         'talented': 5,\n",
       "         'latest': 5,\n",
       "         'details': 5,\n",
       "         'interesting': 5,\n",
       "         'back': 5,\n",
       "         'developers': 5,\n",
       "         'benefits': 5,\n",
       "         'been': 5,\n",
       "         'than': 5,\n",
       "         'able': 5,\n",
       "         'years': 5,\n",
       "         'SaaS': 5,\n",
       "         'already': 5,\n",
       "         'equity': 5,\n",
       "         'sure': 5,\n",
       "         'speak': 5,\n",
       "         'experience.': 5,\n",
       "         'private': 5,\n",
       "         'elite': 5,\n",
       "         'making': 4,\n",
       "         'class': 4,\n",
       "         'including': 4,\n",
       "         'book': 4,\n",
       "         'wish': 4,\n",
       "         'Glasgow': 4,\n",
       "         'meet': 4,\n",
       "         'cutting-edge': 4,\n",
       "         'digital': 4,\n",
       "         'cloud': 4,\n",
       "         'joining': 4,\n",
       "         'expertise': 4,\n",
       "         'system': 4,\n",
       "         'support': 4,\n",
       "         'link': 4,\n",
       "         \"don't\": 4,\n",
       "         'so,': 4,\n",
       "         'now': 4,\n",
       "         'industry.': 4,\n",
       "         'Associate': 4,\n",
       "         'individuals': 4,\n",
       "         'We': 4,\n",
       "         'arrange': 4,\n",
       "         'current': 4,\n",
       "         'convenient': 4,\n",
       "         '+44': 4,\n",
       "         'well.': 4,\n",
       "         'Scotland': 4,\n",
       "         'key': 4,\n",
       "         'information': 4,\n",
       "         'past': 4,\n",
       "         'world': 4,\n",
       "         'Edinburgh': 4,\n",
       "         'offers': 4,\n",
       "         'growth': 4,\n",
       "         'life': 4,\n",
       "         'existing': 4,\n",
       "         'well': 4,\n",
       "         'develop': 4,\n",
       "         'connect': 4,\n",
       "         'due': 4,\n",
       "         'following': 4,\n",
       "         'bank': 4,\n",
       "         '\\\\nI': 4,\n",
       "         'well!': 4,\n",
       "         'last': 4,\n",
       "         'we’re': 4,\n",
       "         'passionate': 4,\n",
       "         'real': 4,\n",
       "         'small': 4,\n",
       "         'complex': 4,\n",
       "         'Partner': 4,\n",
       "         'chat': 4,\n",
       "         'Haskell': 4,\n",
       "         'frontend': 4,\n",
       "         'data': 4,\n",
       "         'after': 4,\n",
       "         'going': 4,\n",
       "         'being': 4,\n",
       "         'Hybrid': 4,\n",
       "         'recently': 4,\n",
       "         'both': 4,\n",
       "         'Django': 4,\n",
       "         'Salary:': 4,\n",
       "         'very': 4,\n",
       "         'alongside': 4,\n",
       "         'standards': 4,\n",
       "         \"Ubaidullah,\\\\n\\\\nI'm\": 4,\n",
       "         'its': 4,\n",
       "         'find': 4,\n",
       "         'In': 4,\n",
       "         '\\\\nThey': 4,\n",
       "         'permanent': 4,\n",
       "         'right': 4,\n",
       "         'recruiter': 4,\n",
       "         'developer': 4,\n",
       "         'start-up': 4,\n",
       "         'partnered': 4,\n",
       "         'Microservices,': 4,\n",
       "         'Financial': 4,\n",
       "         'startup': 4,\n",
       "         'index': 4,\n",
       "         'depending': 4,\n",
       "         'significant': 4,\n",
       "         'distributed': 4,\n",
       "         'directly': 4,\n",
       "         'remote,': 4,\n",
       "         'solving': 4,\n",
       "         'Golang': 4,\n",
       "         'onsite': 4,\n",
       "         'PHP': 4,\n",
       "         'A': 4,\n",
       "         'reply': 4,\n",
       "         'Trading': 4,\n",
       "         'made': 4,\n",
       "         'closely': 4,\n",
       "         'problems': 4,\n",
       "         'believe': 4,\n",
       "         '@': 4,\n",
       "         'health': 4,\n",
       "         'reinsurance': 4,\n",
       "         'mental': 4,\n",
       "         'skills,': 3,\n",
       "         'exam': 3,\n",
       "         'instructor': 3,\n",
       "         '7': 3,\n",
       "         'access': 3,\n",
       "         'miss': 3,\n",
       "         'dedicated': 3,\n",
       "         'questions': 3,\n",
       "         'Please': 3,\n",
       "         'online': 3,\n",
       "         'searching': 3,\n",
       "         'Build': 3,\n",
       "         'nice': 3,\n",
       "         'collaborate': 3,\n",
       "         'impact': 3,\n",
       "         'hybrid': 3,\n",
       "         'Fraser': 3,\n",
       "         'contact': 3,\n",
       "         'may': 3,\n",
       "         'manage': 3,\n",
       "         'responsible': 3,\n",
       "         'ensuring': 3,\n",
       "         'approach': 3,\n",
       "         'pivotal': 3,\n",
       "         'period': 3,\n",
       "         'running': 3,\n",
       "         'value': 3,\n",
       "         'ideas': 3,\n",
       "         'users': 3,\n",
       "         'below': 3,\n",
       "         '\\\\n\\\\nHope': 3,\n",
       "         'year': 3,\n",
       "         'bit': 3,\n",
       "         'mthree': 3,\n",
       "         'provides': 3,\n",
       "         'Bank': 3,\n",
       "         'times': 3,\n",
       "         'required': 3,\n",
       "         'via': 3,\n",
       "         'launching': 3,\n",
       "         'successful': 3,\n",
       "         'virtual': 3,\n",
       "         'invest': 3,\n",
       "         '(0)': 3,\n",
       "         'Hi,\\\\n\\\\nI': 3,\n",
       "         'internal': 3,\n",
       "         'attached': 3,\n",
       "         'Would': 3,\n",
       "         'copy': 3,\n",
       "         'require': 3,\n",
       "         'features': 3,\n",
       "         'brand': 3,\n",
       "         'business.': 3,\n",
       "         'options': 3,\n",
       "         'progression': 3,\n",
       "         'add': 3,\n",
       "         'positions': 3,\n",
       "         'potential': 3,\n",
       "         'us': 3,\n",
       "         'success': 3,\n",
       "         '\\\\n\\\\nWhen': 3,\n",
       "         'future': 3,\n",
       "         'position,': 3,\n",
       "         'wondering': 3,\n",
       "         'were': 3,\n",
       "         'while': 3,\n",
       "         'Digital': 3,\n",
       "         '(and': 3,\n",
       "         'products': 3,\n",
       "         'mind': 3,\n",
       "         'fans': 3,\n",
       "         'experienced': 3,\n",
       "         'understand': 3,\n",
       "         '\\\\n\\\\nIn': 3,\n",
       "         'levels': 3,\n",
       "         'scalable': 3,\n",
       "         'APIs': 3,\n",
       "         'similar': 3,\n",
       "         'search': 3,\n",
       "         '1': 3,\n",
       "         'skilled': 3,\n",
       "         'offering': 3,\n",
       "         'C#,': 3,\n",
       "         'Revolent': 3,\n",
       "         'become': 3,\n",
       "         'companies.': 3,\n",
       "         'continued': 3,\n",
       "         'senior': 3,\n",
       "         'level': 3,\n",
       "         'technologies': 3,\n",
       "         '\\\\n\\\\nYour': 3,\n",
       "         'fantastic': 3,\n",
       "         'still': 3,\n",
       "         'unique': 3,\n",
       "         'multiple': 3,\n",
       "         'touch': 3,\n",
       "         '\\\\n\\\\nHere': 3,\n",
       "         'Helping': 3,\n",
       "         'Solutions': 3,\n",
       "         'bonus': 3,\n",
       "         'education': 3,\n",
       "         'space': 3,\n",
       "         'main': 3,\n",
       "         'language': 3,\n",
       "         'early': 3,\n",
       "         'moving': 3,\n",
       "         \"it's\": 3,\n",
       "         'expand': 3,\n",
       "         '12': 3,\n",
       "         'I’m': 3,\n",
       "         'Junior': 3,\n",
       "         'Web': 3,\n",
       "         'since': 3,\n",
       "         'With': 3,\n",
       "         'continue': 3,\n",
       "         'front': 3,\n",
       "         'Google': 3,\n",
       "         'Working': 3,\n",
       "         'receive': 3,\n",
       "         'match': 3,\n",
       "         'discussing': 3,\n",
       "         'organisation': 3,\n",
       "         'talk': 3,\n",
       "         'regarding': 3,\n",
       "         '\\\\n-': 3,\n",
       "         'Fully': 3,\n",
       "         'Typescript': 3,\n",
       "         'had': 3,\n",
       "         'platform.': 3,\n",
       "         'delivering': 3,\n",
       "         'exclusively': 3,\n",
       "         'Think': 3,\n",
       "         'p/a': 3,\n",
       "         'learning': 3,\n",
       "         'social': 3,\n",
       "         \"they're\": 3,\n",
       "         'Platform': 3,\n",
       "         \"We're\": 3,\n",
       "         'financial': 3,\n",
       "         '(Hybrid': 3,\n",
       "         'full': 3,\n",
       "         'such': 3,\n",
       "         'My': 3,\n",
       "         'delivery': 3,\n",
       "         'Their': 3,\n",
       "         'big': 3,\n",
       "         'hiring': 3,\n",
       "         'technical': 3,\n",
       "         'in,': 3,\n",
       "         'Docker,': 3,\n",
       "         'Services': 3,\n",
       "         'critical': 3,\n",
       "         'backed': 3,\n",
       "         'challenges': 3,\n",
       "         'founding': 3,\n",
       "         'Lead': 3,\n",
       "         'Language': 3,\n",
       "         '\\\\nThis': 3,\n",
       "         'AWS,': 3,\n",
       "         'Design': 3,\n",
       "         'Global': 3,\n",
       "         'professional': 3,\n",
       "         'Understanding': 3,\n",
       "         'Director': 3,\n",
       "         '\\\\n\\\\n': 3,\n",
       "         \"It's\": 3,\n",
       "         'specialise': 3,\n",
       "         'challenging': 3,\n",
       "         'CTO': 3,\n",
       "         'chat?': 3,\n",
       "         '\\\\n\\\\nWould': 3,\n",
       "         'Ogundapo\\\\nSenior': 3,\n",
       "         'track': 3,\n",
       "         'raised': 3,\n",
       "         'Kovalee': 3,\n",
       "         'follow': 3,\n",
       "         'you’d': 3,\n",
       "         'perfect': 3,\n",
       "         'Hedge': 3,\n",
       "         'tackle': 3,\n",
       "         'happy': 3,\n",
       "         'ideally': 3,\n",
       "         'platform,': 3,\n",
       "         'money': 3,\n",
       "         'here:': 3,\n",
       "         'Thanks': 2,\n",
       "         'yourself': 2,\n",
       "         'certified': 2,\n",
       "         'security': 2,\n",
       "         'important': 2,\n",
       "         'Training': 2,\n",
       "         '—': 2,\n",
       "         '11am': 2,\n",
       "         'regularly': 2,\n",
       "         'Live': 2,\n",
       "         'details.': 2,\n",
       "         'Manager': 2,\n",
       "         'Glasgow.': 2,\n",
       "         'regards,\\\\n\\\\n': 2,\n",
       "         'University': 2,\n",
       "         'solutions': 2,\n",
       "         'businesses': 2,\n",
       "         'world.': 2,\n",
       "         \"You'll\": 2,\n",
       "         'deliver': 2,\n",
       "         'customer': 2,\n",
       "         'platforms,': 2,\n",
       "         'solutions.': 2,\n",
       "         'J.P.': 2,\n",
       "         'makes': 2,\n",
       "         'food': 2,\n",
       "         'assets': 2,\n",
       "         'funds': 2,\n",
       "         'sense': 2,\n",
       "         'responsibility': 2,\n",
       "         'combined': 2,\n",
       "         'passion': 2,\n",
       "         'ICG': 2,\n",
       "         'particularly': 2,\n",
       "         'finance': 2,\n",
       "         'way': 2,\n",
       "         'growth.': 2,\n",
       "         'months': 2,\n",
       "         'implement': 2,\n",
       "         'designing': 2,\n",
       "         'coming': 2,\n",
       "         'either': 2,\n",
       "         'completing': 2,\n",
       "         'confidential': 2,\n",
       "         'recruitment': 2,\n",
       "         'placing': 2,\n",
       "         'London.': 2,\n",
       "         'there,': 2,\n",
       "         'pencil': 2,\n",
       "         'programme': 2,\n",
       "         'these': 2,\n",
       "         'remotely': 2,\n",
       "         'Aspire': 2,\n",
       "         'Scholarship,': 2,\n",
       "         'programme.': 2,\n",
       "         'interactive': 2,\n",
       "         'form': 2,\n",
       "         'Full': 2,\n",
       "         'Stack': 2,\n",
       "         'phone': 2,\n",
       "         'understanding': 2,\n",
       "         'situation': 2,\n",
       "         'sounds': 2,\n",
       "         'specialist': 2,\n",
       "         'CGI': 2,\n",
       "         'discussion': 2,\n",
       "         'feel': 2,\n",
       "         'seeking': 2,\n",
       "         'Security': 2,\n",
       "         '5': 2,\n",
       "         'no': 2,\n",
       "         'days,': 2,\n",
       "         'well!\\\\n\\\\nI': 2,\n",
       "         'greenfield': 2,\n",
       "         'development,': 2,\n",
       "         'run': 2,\n",
       "         'in.': 2,\n",
       "         'start': 2,\n",
       "         'Client': 2,\n",
       "         'Back': 2,\n",
       "         'flexibility': 2,\n",
       "         'home': 2,\n",
       "         'Back-End': 2,\n",
       "         'maintain': 2,\n",
       "         'essential': 2,\n",
       "         'along': 2,\n",
       "         'give': 2,\n",
       "         'Frank': 2,\n",
       "         'say': 2,\n",
       "         'contract': 2,\n",
       "         'banking': 2,\n",
       "         'Front': 2,\n",
       "         'areas': 2,\n",
       "         'Test': 2,\n",
       "         'Resourcer': 2,\n",
       "         'wondered': 2,\n",
       "         'finding': 2,\n",
       "         '\\\\n\\\\nDo': 2,\n",
       "         'Hey,': 2,\n",
       "         'check': 2,\n",
       "         'Campbell\\\\nRecruitment': 2,\n",
       "         'ITECCO': 2,\n",
       "         'super': 2,\n",
       "         'impressed': 2,\n",
       "         'background': 2,\n",
       "         'brands': 2,\n",
       "         'built': 2,\n",
       "         'teams,': 2,\n",
       "         'easy': 2,\n",
       "         'winning': 2,\n",
       "         'sports': 2,\n",
       "         'pride': 2,\n",
       "         'incredibly': 2,\n",
       "         'creating': 2,\n",
       "         'collaborative': 2,\n",
       "         'whether': 2,\n",
       "         'strategic': 2,\n",
       "         'you’ll': 2,\n",
       "         'much': 2,\n",
       "         'appreciate': 2,\n",
       "         'it.': 2,\n",
       "         'ambition': 2,\n",
       "         'tier': 2,\n",
       "         'experience,': 2,\n",
       "         '\\\\nAre': 2,\n",
       "         'between': 2,\n",
       "         'keep': 2,\n",
       "         'catch': 2,\n",
       "         'JavaScript,': 2,\n",
       "         'end': 2,\n",
       "         'professionals': 2,\n",
       "         'clients,': 2,\n",
       "         'profile,': 2,\n",
       "         'paid': 2,\n",
       "         'Developer,': 2,\n",
       "         'minutes': 2,\n",
       "         'conversation': 2,\n",
       "         'because': 2,\n",
       "         'mid-level': 2,\n",
       "         'paying': 2,\n",
       "         'Microservices': 2,\n",
       "         'adopt': 2,\n",
       "         'DevOps': 2,\n",
       "         'Cloud': 2,\n",
       "         'flexible': 2,\n",
       "         'Specialising': 2,\n",
       "         'C#': 2,\n",
       "         'week?': 2,\n",
       "         'But': 2,\n",
       "         'growth,': 2,\n",
       "         'tools,': 2,\n",
       "         'employees': 2,\n",
       "         'bring': 2,\n",
       "         'things': 2,\n",
       "         'detail,': 2,\n",
       "         'ASAP': 2,\n",
       "         'on.': 2,\n",
       "         'MySQL,': 2,\n",
       "         'RESTful': 2,\n",
       "         'skills.\\\\n\\\\nWould': 2,\n",
       "         'another': 2,\n",
       "         '10': 2,\n",
       "         'Ubaidullah,\\\\n\\\\nHow': 2,\n",
       "         'recruit': 2,\n",
       "         'leaders': 2,\n",
       "         'used': 2,\n",
       "         'Javascript': 2,\n",
       "         'upskill': 2,\n",
       "         'centre': 2,\n",
       "         'public': 2,\n",
       "         'up,': 2,\n",
       "         'member': 2,\n",
       "         'meetings': 2,\n",
       "         'specialising': 2,\n",
       "         'events': 2,\n",
       "         'Virtual': 2,\n",
       "         'busier': 2,\n",
       "         'grown': 2,\n",
       "         'business,': 2,\n",
       "         'REST': 2,\n",
       "         'operate': 2,\n",
       "         'Remote': 2,\n",
       "         'you,': 2,\n",
       "         'Hays': 2,\n",
       "         'Recruiting': 2,\n",
       "         'different': 2,\n",
       "         'stood': 2,\n",
       "         'So': 2,\n",
       "         'PostgreSQL,': 2,\n",
       "         'competitive': 2,\n",
       "         'option': 2,\n",
       "         'detailed': 2,\n",
       "         'read': 2,\n",
       "         'Location:': 2,\n",
       "         'Stack:': 2,\n",
       "         'Node.js': 2,\n",
       "         'you?\\\\n\\\\nI': 2,\n",
       "         'offer.': 2,\n",
       "         'Toal\\\\nJavaScript': 2,\n",
       "         'Venturi': 2,\n",
       "         'Your': 2,\n",
       "         '\\\\n\\\\nAs': 2,\n",
       "         'Engineer,': 2,\n",
       "         'architecture,': 2,\n",
       "         'includes:': 2,\n",
       "         'Node.JS,': 2,\n",
       "         'Typescript,': 2,\n",
       "         'you!': 2,\n",
       "         'worth': 2,\n",
       "         'fitness': 2,\n",
       "         'mission': 2,\n",
       "         'node.js': 2,\n",
       "         '(Typescript,': 2,\n",
       "         'startup.': 2,\n",
       "         '(1': 2,\n",
       "         \"you'll\": 2,\n",
       "         'Mohammed,': 2,\n",
       "         'talent.io': 2,\n",
       "         'tailored': 2,\n",
       "         'Ubaidullah,\\\\n': 2,\n",
       "         'involved': 2,\n",
       "         'received': 2,\n",
       "         'media': 2,\n",
       "         'solid': 2,\n",
       "         '.NET': 2,\n",
       "         'changing': 2,\n",
       "         '\\\\nIf': 2,\n",
       "         'UK.': 2,\n",
       "         'WFH': 2,\n",
       "         'higher': 2,\n",
       "         'requirement': 2,\n",
       "         'must': 2,\n",
       "         'Kotlin,': 2,\n",
       "         'Job': 2,\n",
       "         'located': 2,\n",
       "         'Sourcer': 2,\n",
       "         'incredible': 2,\n",
       "         'revolutionising': 2,\n",
       "         'process.': 2,\n",
       "         'connects': 2,\n",
       "         'variety': 2,\n",
       "         'network': 2,\n",
       "         'UK,': 2,\n",
       "         'massively': 2,\n",
       "         'reducing': 2,\n",
       "         '\\\\n\\\\nLet': 2,\n",
       "         'shaping': 2,\n",
       "         'customers': 2,\n",
       "         'far': 2,\n",
       "         'engineer,': 2,\n",
       "         'analysis': 2,\n",
       "         'deployment': 2,\n",
       "         'massive': 2,\n",
       "         '(Core': 2,\n",
       "         'Fintech': 2,\n",
       "         'AWS)': 2,\n",
       "         'balance': 2,\n",
       "         'Scale': 2,\n",
       "         'Leading': 2,\n",
       "         'adoption': 2,\n",
       "         'These': 2,\n",
       "         'move': 2,\n",
       "         'commercial': 2,\n",
       "         'framework.': 2,\n",
       "         '\\\\n\\\\nThey': 2,\n",
       "         'firms': 2,\n",
       "         'tools': 2,\n",
       "         'driven': 2,\n",
       "         'Technical': 2,\n",
       "         'Skills': 2,\n",
       "         'NodeJS,': 2,\n",
       "         'React': 2,\n",
       "         'Docker': 2,\n",
       "         'role.': 2,\n",
       "         'detail?\\\\n\\\\nBest': 2,\n",
       "         'PHP,': 2,\n",
       "         'Node': 2,\n",
       "         'monthly': 2,\n",
       "         'considering': 2,\n",
       "         'degree': 2,\n",
       "         'enhance': 2,\n",
       "         'real-world': 2,\n",
       "         'problem-solving': 2,\n",
       "         'those': 2,\n",
       "         'range': 2,\n",
       "         'TEDI-London': 2,\n",
       "         'interested,': 2,\n",
       "         'fast': 2,\n",
       "         'phase': 2,\n",
       "         'Python,': 2,\n",
       "         'Salary': 2,\n",
       "         'Equity': 2,\n",
       "         'fit.': 2,\n",
       "         'Hello': 2,\n",
       "         'base': 2,\n",
       "         'apps': 2,\n",
       "         'apps,': 2,\n",
       "         'App': 2,\n",
       "         'Ubaidullah,\\\\n\\\\nHope': 2,\n",
       "         'funding': 2,\n",
       "         'year.': 2,\n",
       "         'entrepreneurial': 2,\n",
       "         'reporting': 2,\n",
       "         'operating': 2,\n",
       "         'close': 2,\n",
       "         'tomorrow': 2,\n",
       "         '1%': 2,\n",
       "         'Elite': 2,\n",
       "         'Firm': 2,\n",
       "         'fit!\\\\n\\\\nRole:': 2,\n",
       "         'Year': 2,\n",
       "         'Options)\\\\nSkills:': 2,\n",
       "         'agnostic,': 2,\n",
       "         'uses': 2,\n",
       "         'previously': 2,\n",
       "         'records.': 2,\n",
       "         'greatest': 2,\n",
       "         'pushing': 2,\n",
       "         'you?\\\\n•': 2,\n",
       "         'treated': 2,\n",
       "         \"company's\": 2,\n",
       "         '#1': 2,\n",
       "         'asset\\\\n•': 2,\n",
       "         'Low': 2,\n",
       "         'attrition': 2,\n",
       "         'rate;': 2,\n",
       "         'daily': 2,\n",
       "         'basis!\\\\n•': 2,\n",
       "         'Very': 2,\n",
       "         'friendly,': 2,\n",
       "         'tight-knit': 2,\n",
       "         'environment\\\\n•': 2,\n",
       "         'Flat': 2,\n",
       "         'structure,': 2,\n",
       "         'clear': 2,\n",
       "         'route\\\\n\\\\nThis': 2,\n",
       "         'involves': 2,\n",
       "         'tech.\\\\n\\\\nWould': 2,\n",
       "         'chat?\\\\n\\\\nChristopher': 2,\n",
       "         'Lee\\\\nAssociate': 2,\n",
       "         'tell': 2,\n",
       "         'LinkedIn': 2,\n",
       "         'Quant': 2,\n",
       "         'guarantee': 2,\n",
       "         'work-life': 2,\n",
       "         'better': 2,\n",
       "         'chat?\\\\n\\\\nHalimat': 2,\n",
       "         'hands-on': 2,\n",
       "         'revolutionizing': 2,\n",
       "         'mobile': 2,\n",
       "         'Machine': 2,\n",
       "         'leader': 2,\n",
       "         'million': 2,\n",
       "         'impressive': 2,\n",
       "         'Engineer:': 2,\n",
       "         'previous': 2,\n",
       "         'Smith\\\\nHead': 2,\n",
       "         \"\\\\n\\\\nI'm\": 2,\n",
       "         'Fund,': 2,\n",
       "         'pay': 2,\n",
       "         'Expert': 2,\n",
       "         'tech,': 2,\n",
       "         '\\\\n\\\\nCheers,': 2,\n",
       "         '\\\\nLauren\\\\n\\\\nLauren': 2,\n",
       "         'Poelzer\\\\nFinTech': 2,\n",
       "         'MSc': 2,\n",
       "         'KCL': 2,\n",
       "         'confident': 2,\n",
       "         '\\\\n\\\\nTech': 2,\n",
       "         'details,': 2,\n",
       "         'caught': 2,\n",
       "         'organisations': 2,\n",
       "         'lead': 2,\n",
       "         '\\\\nAWS': 2,\n",
       "         '\\\\n\\\\nThis': 2,\n",
       "         'Oakland': 2,\n",
       "         'Search': 2,\n",
       "         'tackling': 2,\n",
       "         'Companies': 2,\n",
       "         'occasional': 2,\n",
       "         'travel': 2,\n",
       "         'Sharma\\\\nProfessional': 2,\n",
       "         'll': 2,\n",
       "         'Acquisition\\\\n': 2,\n",
       "         '\\\\nCapgemini': 2,\n",
       "         'India\\\\nTel.:': 2,\n",
       "         '2038313904': 2,\n",
       "         '(UK)': 2,\n",
       "         '\\\\nwww.capgemini.com': 2,\n",
       "         'AWS.': 2,\n",
       "         'Dear': 2,\n",
       "         'America': 2,\n",
       "         'improving': 2,\n",
       "         'efficiency': 2,\n",
       "         'Risk': 2,\n",
       "         'Analytics': 2,\n",
       "         'improve': 2,\n",
       "         'scaling': 2,\n",
       "         '\\\\nSalary:': 2,\n",
       "         ...})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing for Sentiment Analysis\n",
    "\n",
    "**binary multinomial naive Bayes** or **binary naive Bayes**. For each document we remove all duplicate words before concatenating them into the single big document during training and we also remove duplicates words from the test documents. This is per document binarization so the counts don't have to be binary as it can occur in multiple documents the same word.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes for other text classification tasks\n",
    "\n",
    "**spam detection**\n",
    "\n",
    "\n",
    "\n",
    "for **language id**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes as a Language Model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: Precision, Recall, F-measure\n",
    "\n",
    "to evaluate any system for detecting things we build a **confusion matrix**. A confusion matrix is a table for visualizing how an algorithm performs with respect to the human gold labels, using two dimentions (system output, gold labels).\n",
    "\n",
    "theres a metric called accuracy which asks the question \"what percentage of all the observations our system labelled correctly. This is not a great metric for classification as you may and usually have imbalanced classes in the training set e.g. you may have millions of not spam emails and just a few spam emails which then would make it \"pointless\" to measure the accuracy.\n",
    "\n",
    "You instead use metrics like **precision** and **recall**. \n",
    "\n",
    "- *precision*: number of items that the system deteceted that are in fact positive.\n",
    "\n",
    "precision = (true positives) / (true positives + false positives)\n",
    "\n",
    "- *recall*: measures the percentage of items actually present in the input that were correctly identified by the system.\n",
    "\n",
    "recall = (true positives) / (true positives + false negatives)\n",
    "\n",
    "there are different metrics that combine precision and recall, one popular one is the **F-measure**. This is the formula for the F-measure:\n",
    "\n",
    "```\n",
    "F-measure = (B^2 + 1)PR / (B^2 * P + R)\n",
    "```\n",
    "\n",
    "when B is 1, we get the F1 measure:\n",
    "\n",
    "```\n",
    "F-measure = 2PR / (P + R)\n",
    "```\n",
    "\n",
    "The F-measure comes from the harmonic mean of precision and recall.\n",
    "\n",
    "**How do you evaluate with more than one class?**\n",
    "\n",
    "we could compute the precision of a multiclass classification with naive bayes or any other techinique for example when we classify an email into one of these classes \"urgent\", \"spam\", \"normal\" maybe by using **microaveraging** where we compute the performance of each class and average over all classes.\n",
    "\n",
    "-**macroaveraging**: we compute the performance of each class and average over all classes.\n",
    "-**microaveraging**: we collect the decision for all classes into a single confusion matrix and then compute precision and recall from that matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test sets and Cross-validation\n",
    "\n",
    "we use the **training set** to train the model, then we use the **development test set (dev set)** to tune the parameters\n",
    "\n",
    "A problem having a small data set is that we don't have enough data for training and testing or even when you save a lot of data for trainign and you end up not having enough data for dev/test testing. We can use all of our dataset for training and testing by doing **cross-validation**.\n",
    "\n",
    "In cross validation we choose a number *k* and partition our data into *k* subsets called **folds**. We choose one of these folds as our test test and train our classifier on the remaining k-1 folds and then compute the error rate on the test set. Then we repeat with another fold until *k* times and average the test error rate from these *k* runs to get an average error rate.\n",
    "\n",
    "The problem with cross validation is that we're using all of our data for testing so we can't examine any of the data to suggest possible features and influence performance. so what to do? we can create a fixed training set and test set and do the k-fold cross validation on the training set and compute the error rate on the test set as normal.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Significance Testing\n",
    "\n",
    "how do we know if our system is better than our old one? This is the domain of statistical hypothesis testing.\n",
    "\n",
    "We'll introduce the statistical significance testing for NLP classifiers.\n",
    "\n",
    "we have δ delta (greek symbol) that tells us the difference between two classifiers based on a metric *M* on a test dataset *x*.\n",
    "\n",
    "δ(x) is called the **effect size**. Bigger delta means A is better, small delta means is slighlty better.\n",
    "\n",
    "Why can we not just check that delta is positive? Cause A can be better than B on that particular test test but is it better on a different dataset?\n",
    "\n",
    "**null hypothesis** supposes that δ(x) is actually negative or zero, meaning A is not better than B.\n",
    "\n",
    "the **p-value**. We create a random variable *X* ranging over all test sets and we ask how likely is it, if the null hypothesis was correct, that among these test sets we would encounter the value of δ(x) we found if we repeated the experiment great times?\n",
    "\n",
    "**p-value**: the probability, assuming the null hypothesis is true, of seeing the δ(x) that we saw or one even greater.\n",
    "\n",
    "P(δ(X) >= δ(x) | H0 is true)\n",
    "\n",
    "\n",
    "There are two parameteric tests used in NLP, **approximate randomization** and **bootstrap test**.\n",
    "\n",
    "**paired tests** are those where we compare two sets of observation that are aligned and each observatino in one set can be paired to an observation in another set.\n",
    "\n",
    "**Paired Bootstrap Test**\n",
    "\n",
    "this can be applied to any metric. the term **bootstrapping** means repeatedly drawing large number of samples with replacements (**bootstrap samples**) from an original set. this method makes the assumption that the sample is representative of the population.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding harm in classification\n",
    "\n",
    "**representational harm** caused by systems that discriminate on social groups, e.g. stereotypes.\n",
    "\n",
    "**toxiticy detection** is the task of detecting hate speech, abuse, harassement, etc..\n",
    "\n",
    "false positives could be caused also by bad labelers of course.\n",
    "\n",
    "release a **model card** with your NLP model. A model card has:\n",
    "- training algorithm and parameters\n",
    "- training data sources, motivation, and processing\n",
    "- evaluation data sources, motivatin and processing\n",
    "- intended use and users\n",
    "- model performance across different demographic or other groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
