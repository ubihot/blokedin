{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1a25ad",
   "metadata": {},
   "source": [
    "# Foundational Terminology & Concepts\n",
    "\n",
    "1. What is tokenization, and why is it a fundamental first step in most NLP pipelines?\n",
    "\n",
    "2. Define text normalization. Provide at least three distinct examples of normalization techniques mentioned in the course.\n",
    "\n",
    "3. What is the primary difference between stemming and lemmatization? Which of the two is considered a 'simpler' approach, and what is a potential drawback of that simplicity?\n",
    "\n",
    "4. What is the primary function of a regular expression? In the context of regex, what is a 'capture group' and what is its purpose?\n",
    "\n",
    "5. What is the BPE algorithm? How does it differentiate with the Penn Treebank Tokenization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261b651",
   "metadata": {},
   "source": [
    "# General Concepts & The Processsing Pipeline\n",
    "\n",
    "1. The text introduces \"aboutness\" as a key concept. Consider the initial processing pipeline described: html -> ascii -> text -> vocab. Explain how each step in this pipeline could potentially degrade the information needed to determine a document's \"aboutness.\" What specific information is lost at each stage, and what kind of \"aboutness\" (e.g., topic, sentiment, author intent) becomes harder to discern?\n",
    "\n",
    "2. The provided code snippet for processing a URL includes a hardcoded slice: raw = raw[750:23506]. Why is this a fragile and non-scalable method for content extraction? Propose a more robust strategy using regular expressions or HTML parsing concepts mentioned in the text to isolate the main body content from any given HTML page, and discuss the potential edge cases your new strategy would need to handle.\n",
    "\n",
    "3. The notes question whether a vocabulary needs to be sorted (vocab = sorted(set(words))). Beyond simple human readability, describe a specific downstream NLP task or algorithm where having a sorted vocabulary would be computationally advantageous. Conversely, describe a scenario where the sorting step is an unnecessary overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1142ef",
   "metadata": {},
   "source": [
    "# Tokenization and Normalization\n",
    "\n",
    "1. The text lists several \"tokenization complications\" like contractions, multi-word expressions, and URLs. Choose two: contractions (e.g., what're) and URLs (e.g., http://www.google.com). For a sentiment analysis task, argue for two different tokenization strategies for each. For instance, should what're become [\"what're\"], [\"what\", \"re\"], or [\"what\", \"are\"]? Should a URL be discarded, kept as a single token, or replaced with a placeholder like [URL]? Justify your choices by explaining the potential impact on model performance.\n",
    "\n",
    "2. Explain the fundamental difference between stemming (like the Porter algorithm shown) and lemmatization. Provide an original example of a sentence where a simple Porter stemmer would produce a grammatically incorrect or semantically confusing result, while a true lemmatizer would preserve the meaning. In what type of NLP application would this difference be critically important, and in which might it be negligible?\n",
    "\n",
    "3. The text states, \"Tokenization is tied with named entity recognition.\" Elaborate on this relationship. How could a poorly designed tokenizer that, for example, splits AT&T into [\"AT\", \"&\", \"T\"] or Ph.D. into [\"Ph\", \".\", \"D\", \".\"] negatively impact the performance of a subsequent Named Entity Recognition (NER) model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6db1cd",
   "metadata": {},
   "source": [
    "# Regular Expressions\n",
    "\n",
    "1. The ELIZA chatbot example uses a cascaded series of regex substitutions. Consider the rule: s/.* YOU ARE (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/. Critically analyze this rule.\n",
    "  - Explain the function of the greedy .* at the beginning and end.\n",
    "  - How does the capture group (depressed|sad) and the backreference \\1 work together?\n",
    "  - Describe two distinct user inputs that would be correctly handled by this rule, and two inputs that would \"break\" it or lead to a nonsensical response due to the rule's simplicity.\n",
    "\n",
    "2. You are tasked with writing a regex to find all mentions of monetary values in a text. The values can appear in formats like $50, $50.25, $1,000, and $1,000,000.50. Construct a single, robust regular expression to capture all these variations. Break down your regex and explain the purpose of each component (e.g., escaping special characters, using quantifiers like + or *, handling optional groups with ?, and using character sets).\n",
    "\n",
    "3. The text discusses the trade-off between precision (minimizing false positives) and recall (minimizing false negatives). Imagine you are building a system to automatically redact personal information. You need to redact both email addresses and phone numbers. Which would you prioritize for this specific task: precision or recall? Justify your answer by explaining the real-world consequences of having false positives versus false negatives in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca550fef",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding (BPE) & Subword Tokenization\n",
    "\n",
    "1. The text claims BPE is useful for handling \"unknown words\" (Out-of-Vocabulary problem). Explain, step-by-step, how the BPE algorithm's process of iteratively merging frequent pairs allows it to tokenize a word it has never seen during training (e.g., \"techno-optimism\"). Contrast this with a traditional word-based tokenizer that has a fixed vocabulary. What would the traditional tokenizer do with this word, and why is the BPE approach more flexible?\n",
    "\n",
    "2. Consider the following small training corpus: [\"reader\", \"reads\", \"reading\", \"begin\", \"begins\"]. Manually perform the first three merge operations of the BPE algorithm. For each step, you must show:\n",
    "  - The counts of all adjacent character pairs.\n",
    "  - The pair you choose to merge.\n",
    "  - The new, updated representation of the corpus after the merge.\n",
    "\n",
    "3. The notes mention that WordPiece (used in BERT) has a \"different merge strategy based on likelihood.\" Speculate on what this means. How might a merge strategy based on \"maximizing the likelihood of the training data\" differ from BPE's \"merge the most frequent pair\"? Which approach do you think would be more likely to create subwords that align with meaningful morphological units (like read and ing), and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d63689",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
