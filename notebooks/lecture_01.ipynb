{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture One\n",
    "mainly following the Speech and Language Processing book\n",
    "- Tokenization\n",
    "- Regular Expressions\n",
    "- Edit Distance\n",
    "- BPE, what was before BPE?? what's the BPE paper? include it here\n",
    "\n",
    "### What is text?\n",
    "At the lowest level it's string or streams of characters (or bytes).\n",
    "\n",
    "e.g. it could be an html page.\n",
    "\n",
    "```\n",
    "\"<div class=\"md\"><p>Depends on who it's for I guess? Gender? Age? Nationality? Might help people give you ideas. In general, I'd say something tartan maybe? House of Tartan sells blankets and scarves etc (along with lots of other generic Scottish stuff). If your gifts are for artsy people then you could maybe get some Charles Rennie Mackintosh related gifts (the shop in the Lighthouse sells loads of stuff like that - might be worth having a wander around there in general).</p></div>\"\n",
    "```\n",
    "\n",
    "some of the characters are markup, we can use an HTML parser to remove the tags and focus on the main text. It's easier for JSON, HTML, XML but more difficult for PDF, etc...\n",
    "\n",
    "Aboutness is a key concept in text analytics, e.g. identifying relevant to a search request, classifying a document about its content.\n",
    "\n",
    "What is not important?\n",
    "  filler words: on, who, are, I, be, a, there, you, etc..\n",
    "  other words: general, something, maybe, ...depending on the context\n",
    "\n",
    "\n",
    "Text processing as a pipeline:\n",
    "- html\n",
    "- ascii\n",
    "- text\n",
    "- vocab\n",
    "\n",
    "```python\n",
    "# get the text and trim it to desired content\n",
    "html = urlopen(url).read()\n",
    "raw = nltk.clean_html(html)\n",
    "raw = raw[750:23506]\n",
    "\n",
    "# get the tokens of interest from the raw text\n",
    "tokens = nltk.wordpunct_tokenize(raw)\n",
    "tokens = tokens[20:1834]\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "\n",
    "words = [w.lower() for w in text] # normalization\n",
    "vocab = sorted(set(words)) # unique, and sorted (do we need it sorted??)\n",
    "```\n",
    "\n",
    "Most tasks in text processing require text normalization.\n",
    "\n",
    "1. Segmenting/tokenizing terms in running text. e.g.\n",
    "```\n",
    "\"House of Tartan sells blankets\" -> \"House\", \"of\", \"Tartan\", \"sells\", \"blankets\"\n",
    "```\n",
    "\n",
    "- splitting on whitespace: `\" \" TAB \\n \\r`\n",
    "- splitting on punctuation: `_-.?!,;:\"()'&Â£$`\n",
    "\n",
    "The above is accomplished via regex, i.e. splitting.\n",
    "\n",
    "other languages may be more difficult to tokenize. e.g. japanese\n",
    "\n",
    "2. Normalize or 'canonicalize' tokens into a normal form\n",
    "may \"case fold\" aka lower case text, remove numbers, punctuation, etc..\n",
    "e.g.\n",
    "```\n",
    "\"House\", \"of\", \"Tartan\", \"sells\", \"blankets\"\n",
    "\"house\", \"of\", \"tartan\", \"sell\", \"blanket\"\n",
    "```\n",
    "\n",
    "3. Segment long sequences of tokens (usually into sentences)\n",
    "build a binary classifier (later) and/or use heuristic rules\n",
    "```\n",
    "e.g. [Might, help, people, give, you, ideas, .I'd, say, something, tartan, maybe, ?]\n",
    "-> <s>Might help people give you ideas.</s><s>I'd say something tartan maybe?</s>\n",
    "```\n",
    "\n",
    "##### Text Processing Summary\n",
    "Tokenization is easy but hard to do well.\n",
    "\n",
    "Tokenization complications:\n",
    "- Contractions: what're\n",
    "- Numbers: 555,555.00\n",
    "- Multiword expressions: rock'n' roll\n",
    "- useful puntuation: m.p.h. Ph.D., AT&T\n",
    "- URLs: http://www.google.com\n",
    "- Runaway tokens: #mliscoolweneedtolearnitnow\n",
    "\n",
    "The Pen Treebank defines one common tokenization standard used in NLP tasks.\n",
    "\n",
    "Text normalization (and ususally stemming) are almost always required.\n",
    "\n",
    "Tokenization and normalization are language specific.\n",
    "\n",
    "\n",
    "### Regular Expressions, Tokenization, Edit Distance (maybe/?)\n",
    "ELIZA was a chatbot like program using pattern recognition phrases like \"I need X\" and translate into suitable output like \"what would you do if you got X?\". It didn't know anything about the world, it was like a listener that acts like knowing nothing.\n",
    "\n",
    "One tool to describe text pattern is **regular expression**, for example to extract strings from document.\n",
    "**text normalization** means converting it to a more convenient standard form.\n",
    "**tokenization** separating words or word parts from the text document, e.g. english words are separated by whitespace (not always sufficient).\n",
    "for tweets for example we'd need to tokenize the **emoticons** :) or **hashtag** #nlp.\n",
    "some languages are hard to tokenize as they don't have spaces, e.g. japanese.\n",
    "we'd need sometimes to tokenize subwords, short phrases, letters for large LLMs.\n",
    "another part of text normalization is **lemmatization**, the task of determining that words have the same root, e.g. *sang*, *sung*, *sings* are form of the verb *sing*. *sing* is the common *lemma* of these words. A **lemmatizer** maps from all these to *sing*.\n",
    "**stemming** is a simpler form of lemmatization in which we just strip suffixes from the end of the word.\n",
    "Text normalization also includes **sentence segmentation**, breaking text into sentences using cues like periods or exclamation.\n",
    "The metric **edit distance** measures how similar two strings are based on number of edits (insertion, deletion, substitution) it takes to change one string into another.\n",
    "\n",
    "#### Regular Expressions\n",
    "regex is a language for specifying text search strings, used e.g. in `grep`, `vim`.\n",
    "very useful when we have a **pattern** to search for in a **corpus** to search through.\n",
    "a regex will search through the corpus returning all the pattern matches.\n",
    "the corpus can be a single document or a collection.\n",
    "we'll describe the **extended regular expressions**.\n",
    "\n",
    "regular expression patterns are case sensitive:\n",
    "- concatenation: putting characters in sequence is concatenation, e.g. `/woodchucks/`, `/ubaid/`\n",
    "- range: using the `[-]` (use of [] is required) to indicate from to a range, e.g. `/[A-Z]/`, `/[0-9]/`, `/[a-z]/`\n",
    "- Kleene*: * to say how many of something, means zero or more occurrences\n",
    "- Kleene+: at least one\n",
    "- wildcard: `.` matches any one character\n",
    "- anchors: anchor the regular expressions to a particular place in a string\n",
    "  - `^`   start of line\n",
    "  - `$`   end of line\n",
    "  - `\\b`  word boundary\n",
    "  - '\\B`  non-word boundary\n",
    "- **disjunction** operator: `|` specifies either or, e.g. `dog|cat`\n",
    "- enclosing the sequences with `()` we basically make it like a single character\n",
    "regualar expressions are **greedy** (match as large as possible) by default but we can use **non-greedy** Kleene operators (match as little as possible). `*?` or `+?`\n",
    "\n",
    "an example is trying to match all the words `the` in a document\n",
    "we start with /the/ but we're missing beginning of text for example `The`\n",
    "so we do this /[Tt]he/\n",
    "but that we'll also match `other` `there`\n",
    "so we need word boundary like /\\b[Tt]he\\b/\n",
    "\n",
    "\n",
    "^ the above proccess introduces **false positives**, i.e. matching `other` `there`\n",
    "and also false negatives, i.e. missing correct strings\n",
    "\n",
    "reducing the overall error of the application involves two antagonists efforts:\n",
    "- increasing **precision** (minimizing false positives)\n",
    "- increasing **recall** (minimizing false negatives)\n",
    "\n",
    "\n",
    "an important part of regular expression is in **substitutions**. e.g. in python or in vim the sub operator like `s/colour/color`\n",
    "\n",
    "you can use the number operator \\1 to refer to a matched pattern back.\n",
    "this use of parenthese to store a pattern in memory is called **capture group**, every time a capture group is used, the resulting match is stored in a numbered **register**. And you refer to the captured group via numbers like 1, 2, 3, etc..\n",
    "\n",
    "if we want to use parenthese to not capture the group we can use a non-capturing group via /(?:some|a few) (people|cat) like some \\1/\n",
    "in the example above `\\1` would refer to the second parenthese group, i.e. `(people|cat)`\n",
    "\n",
    "Recall ELIZA, that is using a series of cascade regular expression substitution.\n",
    "e.g. some are these\n",
    "```txt\n",
    "s/.* YOU ARE (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/\n",
    "s/.* YOU ARE (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/\n",
    "s/.* all .*/IN WHAT WAY/\n",
    "s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Thethis * Column 1      Column 3 Column 32423 dogy begin other cat the begun w weeee is 223234 our document we'll search through to find patterns, this is random, will we find anything useful?? We 2\"\n",
    "concatenation_pattern = \"we\"\n",
    "sensitive = \"We\"\n",
    "sensitive_solved = \"[Ww]e\"\n",
    "any_single_digit = \"[1234567890]\"\n",
    "range_pattern = \"[0-9]\" # similar to the above one\n",
    "\n",
    "# can use the [] to specigy that the character cannot be by using the ^ at the beginning\n",
    "cannot_be_pattern = \"[^a]\" # matches any single character expect the character a\n",
    "\n",
    "# we can use the ? to indicate the preceding or nothing, i.e. zero or one instance\n",
    "preceding_or_nothing = \"we?\"\n",
    "\n",
    "\"\"\"\n",
    "/[^A-Z]/    not an upper case letter\n",
    "/[^Ss]/     one character neither S or s\n",
    "/[^.]/      not a period\n",
    "/[e^]/      either e or ^\n",
    "/a^b/       match this pattern specifically a^b\n",
    "/colou?r/   color or colour\n",
    "\"\"\"\n",
    "\n",
    "# zero or more occurrences are defined using the Kleene*\n",
    "zero_or_more = \"we*\"\n",
    "# can also do zero or more of complex patterns\n",
    "zero_or_more_complex = \"[ab]*\"\n",
    "# to specify at least one we can use the plus +\n",
    "at_least_one_pattern = \"we+\"\n",
    "another_least_one_pattern = \"[0-9]+\" # at least a digit\n",
    "\n",
    "wildcard_character_pattern = \"beg.n\"\n",
    "\n",
    "# ^ anchor for beginning of line\n",
    "beginning_of_line_pattern = \"^The\"\n",
    "\n",
    "boundary_word_pattern = \"\\bthe\"\n",
    "\n",
    "either_or_pattern =  \"dog|cat\"\n",
    "\n",
    "act_like_single_character = \"dog(ggy|y)\"\n",
    "\n",
    "columns_pattern = \"(Column [0-9]+ *)*\"\n",
    "\n",
    "\n",
    "# exactly this many occurrences {} and can also specify ranges with {m,n}\n",
    "exactly_this_many_occurrences = \"[Ww]e{4}\"\n",
    "range_of_occurrences_pattern = \"[Ww]e{1,4}\"\n",
    "\n",
    "# special characters like the newline are referred to by using the `\\` or even `*` e.g. /\\*/\n",
    "newline_pattern = \"\\n\"\n",
    "asterisk_pattern = \"\\*\"\n",
    "\n",
    "\n",
    "found_patterns = re.findall(concatenation_pattern, document)\n",
    "print(found_patterns)\n",
    "sensitive_matches = re.findall(sensitive, document)\n",
    "print(sensitive_matches)\n",
    "sensitive_solved_matches = re.findall(sensitive_solved, document)\n",
    "print(sensitive_solved_matches)\n",
    "digit_matches = re.findall(any_single_digit, document)\n",
    "print(digit_matches)\n",
    "range_matches = re.findall(range_pattern, document)\n",
    "print(range_matches)\n",
    "anything_but_a_matches = re.findall(cannot_be_pattern, document)\n",
    "print(anything_but_a_matches)\n",
    "preceding_or_nothing_matches = re.findall(preceding_or_nothing, document)\n",
    "print(preceding_or_nothing_matches)\n",
    "zero_or_more_matches = re.findall(zero_or_more, document)\n",
    "print(zero_or_more_matches)\n",
    "zero_or_more_complex_matches = re.findall(zero_or_more_complex, document)\n",
    "print(zero_or_more_complex_matches)\n",
    "at_least_one_pattern_matches = re.findall(at_least_one_pattern, document)\n",
    "print(at_least_one_pattern_matches)\n",
    "at_least_a_digit_matches = re.findall(another_least_one_pattern, document)\n",
    "print(at_least_a_digit_matches)\n",
    "any_one_character_wildcard_matches = re.findall(wildcard_character_pattern, document)\n",
    "print(any_one_character_wildcard_matches)\n",
    "beginning_of_line_matches = re.findall(beginning_of_line_pattern, document)\n",
    "print(beginning_of_line_matches)\n",
    "boundary_word_matches = re.findall(boundary_word_pattern, document)\n",
    "print(f\"boundary_word_matches {boundary_word_matches}\")\n",
    "either_or_matches = re.findall(either_or_pattern, document)\n",
    "print(either_or_matches)\n",
    "precedence_matches = re.findall(act_like_single_character, document) # doesn't seem to do what I expected\n",
    "print(precedence_matches)\n",
    "column_matches = re.findall(columns_pattern, document)\n",
    "print(column_matches)\n",
    "exactly_this_many_occurrences_matches = re.findall(exactly_this_many_occurrences, document)\n",
    "print(exactly_this_many_occurrences_matches)\n",
    "range_of_occurrences_matches = re.findall(range_of_occurrences_pattern, document)\n",
    "print(range_of_occurrences_matches)\n",
    "asterisk_matches = re.findall(asterisk_pattern, document)\n",
    "print(asterisk_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this could be given as an exercise\n",
    "\n",
    "# more complex example\n",
    "initial_pattern = \"$[0-9]+\" # a dollar followed by a digit to get the price\n",
    "# now let's a decimal point and two digits aftwerwards\n",
    "initial_pattern = \"$[0-9]+\\.[0-9]{2}\" # in the book it's like this instead $[0-9]+\\.[0-9][0-9], is it not the same??\n",
    "# the above pattern matches $199.22 but not $199\n",
    "initial_pattern = \"$[0-9]+(\\.[0-9]{2})?\" # remember `?` which means zero or one\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words\n",
    "what does count as a word? we can decide to treat punctuation as a separate word or not depending on the task, e.g. part of speech tagging.\n",
    "**utterrance** is the spoken correlate of a sentence. \n",
    "e.g. \"I do uh main- mainly business data processing\"\n",
    "there are two kinds **disfluencies**. `main-` broken word is called **fragment**\n",
    "where uh um are called **fillers** or **filled-pauses**. should we consider these as words? depends on the application.\n",
    "\n",
    "to understand better what counts as a word we need to understand **word types**, i.e. number of distinct words in the corpus.\n",
    "word **instances** are the total number of N of running words, equivalent of word tokens in the past.\n",
    "\n",
    "do we consider `They` and `they` as two word types or the same? it depends on the task. e.g. for speech recognition same is fine.\n",
    "\n",
    "the relationship between the word type and the word instance is referred to by **Herdan's law** or **Heaps' law**.\n",
    "\n",
    "cats and cat are two differenct **wordforms** but have the same **lemma**. A lemma  is a set of lexical forms having the same stem. The **wordform** is the full inflected or derived form of the word. \n",
    "\n",
    "for many LLMs we actually use **tokens** using the **tokenization** process. The token can be a word or a part of the word.\n",
    "\n",
    "#### Corpora\n",
    "there are variations genre of text. e.g. from telephone conversations, business meetings, medical interviews, etc..\n",
    "to understand what a corpus was meant for is thanks to **datasheet** or **data statements** that includes:\n",
    "- motivation for collecting the corpus\n",
    "- situation: when and in what situation was text written/spoken\n",
    "- language variety: what language was the corpus in?\n",
    "- speaker demographics: what was e.g the age, sex of the text's authors?\n",
    "- collection process: how big is the data? if it is a subsample how was it sampled? was the data collected with consent? how was the data preprocessed? and what metadata is available?\n",
    "- annotation process: what are the annotations, how was the data annotated? how was the annotation process?\n",
    "- distribution: are there copyright or other intellectual property restrictions?\n",
    "\n",
    "\n",
    "#### Text Normalization\n",
    "before any natural language processing of a text, the text has to be normalized through the **text normalization** process which involves:\n",
    "- tokenization (segmentation) words\n",
    "- normalizing word formats\n",
    "- segmenting sentences\n",
    "\n",
    "\n",
    "NOTE: ok at this point we can start collecting the data and do some of text normalization process in a jupyter notebook.\n",
    "\n",
    "\n",
    "**NOTE what happens when you have a corpus with lots of examples of one label??? we need to explain it in our lectures**\n",
    "\n",
    "**the other thing we need to deal with the preprocessing or processing of our corpus for our task is the handling of links, we could potentially neglect them, i.e. remove them with a regular expression so we basically showcase the use of how regex are used, we could do that in one type of classification or in the other one we could just keep it or replace with a placeholder like [LINK], we need to keep the script so that it has both versions, for now we don't do anything special.**\n",
    "\n",
    "- For the above mentioned we should compare the performance of the different solutions.\n",
    "- Create features engineers.\n",
    "- Create synthetic data after training the model.\n",
    "- We could also have one model trained by giving the subject as one of the features.\n",
    "- We've also included the has_attachment feature and the subject.\n",
    "- Ideally also when we create the synthetic data we want to be able to generate these features too so in the lecture descriptions we can explain how the original data looked like.\n",
    "- We should train with capital or lowercase only and se the diffrence and show the difference.**\n",
    "\n",
    "**how do we deal with the fact that our corpus doesn't have all the words that exists in english vocabulary???**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after having run the `../scripts/process_linkedin_messages.py`, we now try to apply the lessons from the first lecture to process the text.\n",
    "\n",
    "Let's first create a corpus by combining all the messages into a one large document. Then extract all the words in the documnet to form the vocabulary.\n",
    "\n",
    "First thing we need to do is to replace many spaces in the individual messages to just one space. The reason we have many spaces is cause of the way we collected the data and put into `scripts/messages.py`.\n",
    "\n",
    "\n",
    "By using the pattern `\\S+` we basically are defininig and implementing a simple version of **tokenization**.\n",
    "For now, we count as a word any non-whitespace consecutive characters.\n",
    "\n",
    "NOTE: we haven't preprocessed the text in any way, i.e. we haven't lowercased anything, taken the lemmas, etc..\n",
    "\n",
    "\n",
    "The below code is a simple example of a **token learner**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "messages_dataset_filepath = \"../datasets/messages.csv\"\n",
    "\n",
    "def simple_token_learner():\n",
    "  words = defaultdict(int)\n",
    "  with open(messages_dataset_filepath, \"r\") as f:\n",
    "    any_non_white_space_character_pattern = \"\\S+\"\n",
    "    lines = f.readlines()\n",
    "\n",
    "    #print(f\"lines={lines}\")\n",
    "    for i, line in enumerate(lines):\n",
    "      #print(f\"line={line}\")\n",
    "      if i == 0: continue # skip the first csv header row\n",
    "      block = int(line[0])\n",
    "      content = line[2:]\n",
    "      #print(f\"type of block is {type(block)}\") \n",
    "      #print(f\"content={content}\")\n",
    "      any_non_white_space_matches = re.findall(any_non_white_space_character_pattern, content)\n",
    "      for word in any_non_white_space_matches:\n",
    "        words[word] += 1\n",
    "      #print(f\"words={any_non_white_space_matches}\")\n",
    "\n",
    "  # sort the words by the most frequent one first\n",
    "  words = dict(sorted(words.items(), key=lambda item: item[1], reverse=True))\n",
    "  for word, freq in words.items():\n",
    "    print(f\"word={word} {freq}\")\n",
    "  #print(words)\n",
    "  print(f\"total words are {len(words)}\")\n",
    "  print(f\"Ubaidullah, freq = {words['Ubaidullah,']}\")\n",
    "\n",
    "  return words\n",
    "\n",
    "\n",
    "\n",
    "words = simple_token_learner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word and Subword Tokenization\n",
    "In NLP, we usally break words into **subword tokens**, which can be words or part of words or individual letters.\n",
    "\n",
    "Tokenization is run before any other language processing.\n",
    "\n",
    "**Top Down (Rule Based) Tokenization**\n",
    "In NLP, we usually keep the punctuations and numbers. Then we need to account for hashtags, urls, emails, dates, special chars in words like AT&T, prices (e.g. $45.45), etc...\n",
    "\n",
    "Can use tokenizer to expand **clitic** contractions marked by apstrophes, e.g. `what're` into `what are`.\n",
    "A clitic is a part of word that cannot stand on its own.\n",
    "\n",
    "Tokenization is tied with **named entity recognition**, task of detecting names, dates and organizations.\n",
    "\n",
    "A common tokenization standard is the **Penn Treebank Tokenization**, where `doesn't` becomes `does n't`.\n",
    "\n",
    "Word tokenization is more comlex in Chinese and Thai where there are no spaces and the words are composed of characters called **hanzi** (Chinese).\n",
    "Each character represents a single unit of meaning (**morpheme**).\n",
    "\n",
    "For some languages like thai we need more than one character to use as a word.\n",
    "\n",
    "**Byte-Pair Encoding: A Bottom Up Tokenization Algorithm**\n",
    "We can use the data to tell us what the words should be unlike the previous approach where we either used a character or whitespace or sth more complex. This is very useful to deal with unknown words, very common in NLP.\n",
    "In NLP, algos learn facts from one corpus (**training** corpus) and use theses facts to make decisions about separate **test** corpus, hence the problem of unknown words.\n",
    "\n",
    "To solve this problem, tokenizers try to induce **subwords** tokens.\n",
    "\n",
    "Most tokenizer scheme have 2 parts:\n",
    "- **token learner**: takes raw training corpus and induces a vocabulary, a set of tokens.\n",
    "- **token segmenter**: takes raw test sentence and segments it into the tokens vocabulary.\n",
    "\n",
    "Two algorithms are used:\n",
    "- **byte pair encoding** (Sennrich et al., 2016)\n",
    "- **unigram language model** (Kudo, 2018)\n",
    "\n",
    "**SentencePiece** (Kudo and Richardson, 2018a) has both implementation, but SentencePiece is usually referred to mean as **unigram language model**.\n",
    "\n",
    "\n",
    "#### BPE\n",
    "- begin with a vocabulary that is all the individual characters.\n",
    "- examine training corpus\n",
    "- choose the two symbols that are most frequently adjacent, e.g. 'A' and 'B'\n",
    "- add the merged symbol 'AB' to the vocabulary\n",
    "- replace every adjacent 'A' 'B' with 'AB'\n",
    "- continue to count and merge creating longer and longer character strings, until k merges have been creating k novel tokens\n",
    "- k thus becomes the parameter of the algorithm\n",
    "- the resulting vocabulary consists of the original characters plus k new symbols.\n",
    "\n",
    "TODO\n",
    "- implement the BPE algorithm in ocaml\n",
    "- go through the karpathy implementation of BPE in its video https://youtu.be/zduSFxRajkE\n",
    "\n",
    "\n",
    "Another tokenization is the Wordpiece which is similar to BPE but uses different merge stratedfy based on likelihood. It's used in BERT and RoBERTa. Slightly more complex than BPE.\n",
    "\n",
    "\n",
    "\n",
    "#### Word Normalization, Lemmatization and Stemming\n",
    "The simplest case of word normalization is **case folding**. e.g. mapping everything to lowercase like `Woodchuck` and `woodchuck` are represented identically, which is very helpful for generalization in tasks like information retrieval or speech recognition.\n",
    "\n",
    "For sentiment analysis and other text classification tasks, information extraction, machine translation instead case folding is generally not done.\n",
    "\n",
    "If you use BPE you may not need to do any other normalization.\n",
    "\n",
    "\n",
    "**lemmatization**\n",
    "is the task of determining that two words have the same root. e.g. one application of it could be `He is reading detective stories` -> `He be read detective story`\n",
    "\n",
    "How is lemmatization done?\n",
    "**morphology** is the study of the of the way words are built up from smaller meaning-bearing units falled **morphemes**.\n",
    "**stem** is the central morpheme of the word, gives the main meaning\n",
    "**affixes** adding additional meanings of various kind.\n",
    "e.g. fox is one morpheme; cats is two `cat` and `s`\n",
    "\n",
    "Lemmatization algos can be complex, hence we can use a simpler morphological analysis called **stemming**, i.e. chopping off the final affixes.\n",
    "**Porter Stemming** consists of rewrite rules run in a series. Not commonly used now cause of overgeneralization and undergeneralization.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very simple Porter Stemming implementation (do not use in production)\n",
    "def porter_stemming(tokens:list[str]):\n",
    "  # Rules ordered by longest suffix first to ensure correct application\n",
    "  rules = [\n",
    "    (\"sses\", \"ss\"),\n",
    "    (\"ies\", \"i\"),\n",
    "    (\"ss\", \"ss\"),\n",
    "    (\"s\", \"\"),\n",
    "    (\"ational\", \"ate\"),\n",
    "    (\"izer\", \"ize\"),\n",
    "    (\"ator\", \"ate\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"able\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    # there are more complex not covered here\n",
    "  ]\n",
    "\n",
    "  stemmed_tokens = []\n",
    "  for token in tokens:\n",
    "    stemmed = token\n",
    "    for suffix, replacement in rules:\n",
    "      if stemmed.endswith(suffix):\n",
    "        stemmed = stemmed[:-len(suffix)] + replacement\n",
    "        break\n",
    "    stemmed_tokens.append(stemmed)\n",
    "  return stemmed_tokens\n",
    "\n",
    "\n",
    "tokens = [\"caresses\", \"ponies\", \"caress\", \"cats\", \"running\", \"horses\", \"relational\", \"digitizer\", \"operator\", \"revival\", \"adjustable\", \"activate\"]\n",
    "expected = [\"caress\", \"poni\", \"caress\", \"cat\", \"running\", \"horse\", \"relate\", \"digitize\", \"operate\", \"reviv\", \"adjust\", \"activ\"]\n",
    "output = porter_stemming(tokens)\n",
    "assert output == expected, f\"expected {expected} but got {output}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE implementation\n",
    "# is words text or is it just actual words can we give it text instead, I feel like it's just text rather than words\n",
    "def get_unique_chars(words):\n",
    "  print(words)\n",
    "  vocabulary = set()\n",
    "  for word in words:\n",
    "    for c in word:\n",
    "      vocabulary.add(c)\n",
    "  return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/karpathy/minbpe/blob/master/minbpe/base.py#L13\n",
    "def get_stats(words:list[list[str]]):\n",
    "  word_counts = defaultdict(int)\n",
    "  for word in words:\n",
    "    for pair in zip(word, word[1:]):\n",
    "      word_counts[pair] += 1\n",
    "  return word_counts\n",
    "\n",
    "\n",
    "# do we need to remove it from vocab?? I don't think so but then how do we do it??\n",
    "# hmm we need to update the corpus after the merge of the tokens into the vocab\n",
    "# so the question is how do we set up the vocab in first place??\n",
    "def merge_most_frequent_tokens_and_get_them(stats, vocabulary):\n",
    "  tokens, freq = stats[0]\n",
    "  token1, token2 = tokens\n",
    "  vocabulary.add(token1+token2)\n",
    "  return token1, token2, token1+token2\n",
    "\n",
    "def update_corpus(tokenized_corpus, new_token):\n",
    "  new_corpus = []\n",
    "  for word in tokenized_corpus:\n",
    "    new_word = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "      # with len(word) - 1 we make sure that there are enough tokens in the future to look for\n",
    "      if i < len(word) - 1 and word[i] + word[i+1] == new_token: # these two tokens basically are forming the new_token\n",
    "        new_word.append(new_token)\n",
    "        i += 2 # skip the next token as it's merged\n",
    "      else:\n",
    "        new_word.append(word[i])\n",
    "        i += 1\n",
    "    new_corpus.append(new_word)\n",
    "  return new_corpus \n",
    "\n",
    "\n",
    "def bpe(corpus, k_merges=10):\n",
    "  merges = []\n",
    "  #print(f\"corpus={corpus}\")\n",
    "  tokenized_corpus = [list(word) for word in corpus] # [[c], [i], [m], [k], [g]]\n",
    "  print(f\"tokenized_corpus={tokenized_corpus}\")\n",
    "  # count the adjacent words and get the most frequent one\n",
    "  vocabulary = get_unique_chars(tokenized_corpus)\n",
    "  #print(len(vocabulary))\n",
    "  #print(vocabulary)\n",
    "\n",
    "  for k in range(k_merges):\n",
    "    stats = get_stats(tokenized_corpus)\n",
    "    #print(stats)\n",
    "    # now sort the stats and get the most frequent occurrence\n",
    "    stats = list(sorted(stats.items(), key=lambda item: item[1], reverse=True))\n",
    "    #print(stats)\n",
    "\n",
    "    token1, token2, new_token = merge_most_frequent_tokens_and_get_them(stats, vocabulary)\n",
    "    #print(len(vocabulary))\n",
    "    #print(vocabulary)\n",
    "    print(f\"most frequent tokens are {token1, token2}\")\n",
    "    merges.append(new_token)\n",
    "\n",
    "    tokenized_corpus = update_corpus(tokenized_corpus, new_token)\n",
    "    #print(f\"updated_corpus={tokenized_corpus}\")\n",
    "\n",
    "  return vocabulary, merges\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, merges = bpe(words, 100)\n",
    "print(f\"vocabulary after bpe = {vocabulary}\")\n",
    "print(f\"merges={merges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Segmentation\n",
    "We've learned our vocabulary, we can now use a token segmenter.\n",
    "\n",
    "The token segmenter run just on the merges we have learned from the training data on the test data.\n",
    "\n",
    "Usually by using punctuation, e.g. periods, question marks, exclamation. Period is more ambiguous.\n",
    "Sentence tokenization woks by deciding (machine learning or rule) whether period is part of word or is sentence boundary marker, an abbreviation dictionary can help find abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_segmenter(merged_vocabulary, sentence, k_merges=10):\n",
    "  words = sentence.split(\" \")\n",
    "  tokenized_sentence = [list(word) for word in words]\n",
    "  print(f\"tokenized_sentence = {tokenized_sentence}\")\n",
    "  tokens = []\n",
    "  for _ in range(k_merges):\n",
    "    for word in tokenized_sentence:\n",
    "      for pair in zip(word, word[1:]):\n",
    "        potential_token = pair[0]+pair[1]\n",
    "        #print(f\"potential_token={potential_token}\")\n",
    "        if potential_token in merged_vocabulary: # need to change the name to merged_vocabulary\n",
    "          tokenized_sentence = update_corpus(tokenized_sentence, potential_token) # what goes into the new_token??\n",
    "  return tokenized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Of course in real settings BPE is run with many Ubaid thousands of merges on a very large input corpus. The result is that most words will be represented as full symbols, and only the very rare words (and unknown words) will have to be represented by their parts.\"\n",
    "\n",
    "\n",
    "sentence_tokens = token_segmenter(merges, sentence, 100)\n",
    "print(sentence_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimun Edit Distance\n",
    "In NLP one common task is to measure how similar two strings are, e.g. graffe with giraffe.\n",
    "Another example is **coreference**, i.e. decide whether two strings refers to the same entity.\n",
    "e.g.\n",
    "```\n",
    "Stanford Arizona Cactus Garden.\n",
    "Stanford University Arizona Cactus Garden.\n",
    "```\n",
    "another task of strings similarity is in the quality measure of transcription produced by a speech recognition system, words that differ by a lot have worse quality transcription and those that differ by a few have better quality.\n",
    "\n",
    "**Edit distance** gives us the technique to quantify these intuitions about similarity.\n",
    "**Minimum edit distance** is defined as the minimum number of editing operations (insertions, deletions, substitutions) needed to transform one string into another.\n",
    "We can even assign even a cost to these operations when doing alignment.\n",
    "The **Levenshtein** distance between two sequences is in which each of these three operations has a cost of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do case folding but for the purpose of this series we don't need to do case folding but keep the representation as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now extract the text and put it into a file and this would become our corpus for the next lectures or we could use the shakespeare corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus_from_messages():\n",
    "  messages_dataset_filepath = \"../datasets/train.csv\"\n",
    "  corpus_filepath = \"../datasets/messages_corpus.txt\"\n",
    "\n",
    "  with open(messages_dataset_filepath, \"r\") as messages_file:\n",
    "    lines = messages_file.readlines()\n",
    "\n",
    "    with open(corpus_filepath, \"w\", newline=\"\") as corpus_file:\n",
    "      for line in lines[1:]:\n",
    "        text = line[2:]\n",
    "        print(text)\n",
    "        corpus_file.write(text)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "extract_corpus_from_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is the **datasheet** of our application blokedin? include it here and assign it as an exercise.\n",
    "\n",
    "TODO:\n",
    "- implement better more real world BPE (ocaml) and reference it in this lecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "we covered\n",
    "- regular expression: a powerful tool for pattern matching\n",
    "- **concatenation** of symbols, **disjunction** ([], |), **counters** (*, +, {n,m}), **anchors** (^, $) and precedence operators ((,)).\n",
    "- **word tokenization and normalization** \n",
    "- **Porter** simplest algorithm for stemming\n",
    "- **minimum edit distance** using **dynamic programming** and **alignment** of two strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAD Course Objectives\n",
    "\n",
    "when each of these done, mark them as accomplished.\n",
    "\n",
    "\n",
    "- **Introduction to text**: Tokenization, vector distribution, cosine similarity, and lemmatization\n",
    "- **Distributions and clustering**: Term distribution, TF-IDF, usupervised text clustering\n",
    "- **Language modeling**: Basic probability, language models as text, smoothing, probabilistic documents similiraty\n",
    "- **Word embeddings**: Dense text representations and word embeddings\n",
    "- **Text classification**: Classification and regression, naive bayes, support vector machines, & information theory\n",
    "- **Natural Language Processing (NLP)**: Sequence tagging and structured parsing\n",
    "- **Evaluation**: Metrics, cross-fold validation, and best practices in model design\n",
    "- **Advanced clustering**: LSI and visualization\n",
    "- **Applications I**: information extraction and question answering\n",
    "- **Applications II**: Dialogue systems and chatbots\n",
    "\n",
    "##### Tech\n",
    "- NLTK, Spacy: for text analysis\n",
    "- Scikit (examples tab, user guide tab): for machine learning\n",
    "- pandas, numpy, scipy: for data analysis\n",
    "- gensim: for topic and word modeling\n",
    "- TODO add anything else that will be used here\n",
    "- \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz \n",
    "\n",
    "> Tokenization is simply the process of splitting text into words on space characters?\n",
    "- True\n",
    "- False\n",
    "\n",
    "> A one hot encoding records the frequency of each word in a piece of text?\n",
    "- True\n",
    "- False\n",
    "\n",
    "> We must store the offset of words in the vectors using a dictionary in order to implement one-hot encoding?\n",
    "- True\n",
    "- False\n",
    "\n",
    "> Why is stemming useful?\n",
    "1. it allows matching of words that sound the same\n",
    "2. it allows matching of words with morphological variations\n",
    "3. it allows matching of missplled words\n",
    "\n",
    "> A vocabulary in a one hot encoding tipycally includes:\n",
    "1. All unique tokens in the text collection\n",
    "2. All normalized unique tokens in the text collection\n",
    "3. All normalized unique tokens with their counts\n",
    "4. All unique tokens with their lemmas\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
