{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'we', 'we']\n",
      "['We']\n",
      "['we', 'we', 'we', 'We']\n",
      "['1', '3', '3', '2', '4', '2', '3', '2', '2', '3', '2', '3', '4', '2']\n",
      "['1', '3', '3', '2', '4', '2', '3', '2', '2', '3', '2', '3', '4', '2']\n",
      "['T', 'h', 'e', 't', 'h', 'i', 's', ' ', '*', ' ', 'C', 'o', 'l', 'u', 'm', 'n', ' ', '1', ' ', ' ', ' ', ' ', ' ', ' ', 'C', 'o', 'l', 'u', 'm', 'n', ' ', '3', ' ', 'C', 'o', 'l', 'u', 'm', 'n', ' ', '3', '2', '4', '2', '3', ' ', 'd', 'o', 'g', 'y', ' ', 'b', 'e', 'g', 'i', 'n', ' ', 'o', 't', 'h', 'e', 'r', ' ', 'c', 't', ' ', 't', 'h', 'e', ' ', 'b', 'e', 'g', 'u', 'n', ' ', 'w', ' ', 'w', 'e', 'e', 'e', 'e', ' ', 'i', 's', ' ', '2', '2', '3', '2', '3', '4', ' ', 'o', 'u', 'r', ' ', 'd', 'o', 'c', 'u', 'm', 'e', 'n', 't', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 's', 'e', 'r', 'c', 'h', ' ', 't', 'h', 'r', 'o', 'u', 'g', 'h', ' ', 't', 'o', ' ', 'f', 'i', 'n', 'd', ' ', 'p', 't', 't', 'e', 'r', 'n', 's', ',', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'r', 'n', 'd', 'o', 'm', ',', ' ', 'w', 'i', 'l', 'l', ' ', 'w', 'e', ' ', 'f', 'i', 'n', 'd', ' ', 'n', 'y', 't', 'h', 'i', 'n', 'g', ' ', 'u', 's', 'e', 'f', 'u', 'l', '?', '?', ' ', 'W', 'e', ' ', '2']\n",
      "['w', 'we', 'we', 'w', 'we']\n",
      "['w', 'weeee', 'we', 'w', 'we']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'b', '', '', '', '', '', '', '', '', '', '', '', '', 'a', '', '', '', '', '', '', 'b', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'a', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'a', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'a', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'a', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['weeee', 'we', 'we']\n",
      "['1', '3', '32423', '223234', '2']\n",
      "['begin', 'begun']\n",
      "['The']\n",
      "boundary_word_matches []\n",
      "['dog', 'cat']\n",
      "['y']\n",
      "['', '', '', '', '', '', '', '', '', '', 'Column 32423 ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['weeee']\n",
      "['weeee', 'we', 'we', 'We']\n",
      "['*']\n"
     ]
    }
   ],
   "source": [
    "document = \"Thethis * Column 1      Column 3 Column 32423 dogy begin other cat the begun w weeee is 223234 our document we'll search through to find patterns, this is random, will we find anything useful?? We 2\"\n",
    "concatenation_pattern = \"we\"\n",
    "sensitive = \"We\"\n",
    "sensitive_solved = \"[Ww]e\"\n",
    "any_single_digit = \"[1234567890]\"\n",
    "range_pattern = \"[0-9]\" # similar to the above one\n",
    "\n",
    "# can use the [] to specigy that the character cannot be by using the ^ at the beginning\n",
    "cannot_be_pattern = \"[^a]\" # matches any single character expect the character a\n",
    "\n",
    "# we can use the ? to indicate the preceding or nothing, i.e. zero or one instance\n",
    "preceding_or_nothing = \"we?\"\n",
    "\n",
    "\"\"\"\n",
    "/[^A-Z]/    not an upper case letter\n",
    "/[^Ss]/     one character neither S or s\n",
    "/[^.]/      not a period\n",
    "/[e^]/      either e or ^\n",
    "/a^b/       match this pattern specifically a^b\n",
    "/colou?r/   color or colour\n",
    "\"\"\"\n",
    "\n",
    "# zero or more occurrences are defined using the Kleene*\n",
    "zero_or_more = \"we*\"\n",
    "# can also do zero or more of complex patterns\n",
    "zero_or_more_complex = \"[ab]*\"\n",
    "# to specify at least one we can use the plus +\n",
    "at_least_one_pattern = \"we+\"\n",
    "another_least_one_pattern = \"[0-9]+\" # at least a digit\n",
    "\n",
    "wildcard_character_pattern = \"beg.n\"\n",
    "\n",
    "# ^ anchor for beginning of line\n",
    "beginning_of_line_pattern = \"^The\"\n",
    "\n",
    "boundary_word_pattern = \"\\bthe\"\n",
    "\n",
    "either_or_pattern =  \"dog|cat\"\n",
    "\n",
    "act_like_single_character = \"dog(ggy|y)\"\n",
    "\n",
    "columns_pattern = \"(Column [0-9]+ *)*\"\n",
    "\n",
    "\n",
    "# exactly this many occurrences {} and can also specify ranges with {m,n}\n",
    "exactly_this_many_occurrences = \"[Ww]e{4}\"\n",
    "range_of_occurrences_pattern = \"[Ww]e{1,4}\"\n",
    "\n",
    "# special characters like the newline are referred to by using the `\\` or even `*` e.g. /\\*/\n",
    "newline_pattern = \"\\n\"\n",
    "asterisk_pattern = \"\\*\"\n",
    "\n",
    "\n",
    "found_patterns = re.findall(concatenation_pattern, document)\n",
    "print(found_patterns)\n",
    "sensitive_matches = re.findall(sensitive, document)\n",
    "print(sensitive_matches)\n",
    "sensitive_solved_matches = re.findall(sensitive_solved, document)\n",
    "print(sensitive_solved_matches)\n",
    "digit_matches = re.findall(any_single_digit, document)\n",
    "print(digit_matches)\n",
    "range_matches = re.findall(range_pattern, document)\n",
    "print(range_matches)\n",
    "anything_but_a_matches = re.findall(cannot_be_pattern, document)\n",
    "print(anything_but_a_matches)\n",
    "preceding_or_nothing_matches = re.findall(preceding_or_nothing, document)\n",
    "print(preceding_or_nothing_matches)\n",
    "zero_or_more_matches = re.findall(zero_or_more, document)\n",
    "print(zero_or_more_matches)\n",
    "zero_or_more_complex_matches = re.findall(zero_or_more_complex, document)\n",
    "print(zero_or_more_complex_matches)\n",
    "at_least_one_pattern_matches = re.findall(at_least_one_pattern, document)\n",
    "print(at_least_one_pattern_matches)\n",
    "at_least_a_digit_matches = re.findall(another_least_one_pattern, document)\n",
    "print(at_least_a_digit_matches)\n",
    "any_one_character_wildcard_matches = re.findall(wildcard_character_pattern, document)\n",
    "print(any_one_character_wildcard_matches)\n",
    "beginning_of_line_matches = re.findall(beginning_of_line_pattern, document)\n",
    "print(beginning_of_line_matches)\n",
    "boundary_word_matches = re.findall(boundary_word_pattern, document)\n",
    "print(f\"boundary_word_matches {boundary_word_matches}\")\n",
    "either_or_matches = re.findall(either_or_pattern, document)\n",
    "print(either_or_matches)\n",
    "precedence_matches = re.findall(act_like_single_character, document) # doesn't seem to do what I expected\n",
    "print(precedence_matches)\n",
    "column_matches = re.findall(columns_pattern, document)\n",
    "print(column_matches)\n",
    "exactly_this_many_occurrences_matches = re.findall(exactly_this_many_occurrences, document)\n",
    "print(exactly_this_many_occurrences_matches)\n",
    "range_of_occurrences_matches = re.findall(range_of_occurrences_pattern, document)\n",
    "print(range_of_occurrences_matches)\n",
    "asterisk_matches = re.findall(asterisk_pattern, document)\n",
    "print(asterisk_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this could be given as an exercise\n",
    "\n",
    "# more complex example\n",
    "initial_pattern = \"$[0-9]+\" # a dollar followed by a digit to get the price\n",
    "# now let's a decimal point and two digits aftwerwards\n",
    "initial_pattern = \"$[0-9]+\\.[0-9]{2}\" # in the book it's like this instead $[0-9]+\\.[0-9][0-9], is it not the same??\n",
    "# the above pattern matches $199.22 but not $199\n",
    "initial_pattern = \"$[0-9]+(\\.[0-9]{2})?\" # remember `?` which means zero or one\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after having run the `../scripts/process_linkedin_messages.py`, we now try to apply the lessons from the first lecture to process the text.\n",
    "\n",
    "Let's first create a corpus by combining all the messages into a one large document. Then extract all the words in the documnet to form the vocabulary.\n",
    "\n",
    "First thing we need to do is to replace many spaces in the individual messages to just one space. The reason we have many spaces is cause of the way we collected the data and put into `scripts/messages.py`.\n",
    "\n",
    "\n",
    "By using the pattern `\\S+` we basically are defininig and implementing a simple version of **tokenization**.\n",
    "For now, we count as a word any non-whitespace consecutive characters.\n",
    "\n",
    "NOTE: we haven't preprocessed the text in any way, i.e. we haven't lowercased anything, taken the lemmas, etc..\n",
    "\n",
    "\n",
    "The below code is a simple example of a **token learner**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "messages_dataset_filepath = \"../datasets/train.csv\"\n",
    "\n",
    "def simple_token_learner():\n",
    "  words = defaultdict(int)\n",
    "  with open(messages_dataset_filepath, \"r\") as f:\n",
    "    any_non_white_space_character_pattern = \"\\S+\"\n",
    "    lines = f.readlines()\n",
    "\n",
    "\n",
    "    #print(f\"lines={lines}\")\n",
    "    for i, line in enumerate(lines):\n",
    "      #print(f\"line={line}\")\n",
    "      if i == 0: continue\n",
    "      block = int(line[0])\n",
    "      text = line[2:]\n",
    "      #print(f\"type of block is {type(block)}\") \n",
    "      #print(f\"text={text}\")\n",
    "      any_non_white_space_matches = re.findall(any_non_white_space_character_pattern, text)\n",
    "      for word in any_non_white_space_matches:\n",
    "        words[word] += 1\n",
    "      #print(f\"words={any_non_white_space_matches}\")\n",
    "\n",
    "    # sort the words by the most frequent one first\n",
    "    words = dict(sorted(words.items(), key=lambda item: item[1], reverse=True))\n",
    "    for word, freq in words.items():\n",
    "      print(f\"word={word} {freq}\")\n",
    "    #print(words)\n",
    "    print(f\"total words are {len(words)}\")\n",
    "    print(f\"Ubaidullah, freq = {words['Ubaidullah,']}\")\n",
    "\n",
    "  return words\n",
    "\n",
    "\n",
    "\n",
    "words = simple_token_learner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: We need to implement a **token segmenter**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE implementation\n",
    "def get_unique_chars(words):\n",
    "  vocabulary = set()\n",
    "  for word in words:\n",
    "    for c in word:\n",
    "      vocabulary.add(c)\n",
    "  return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/karpathy/minbpe/blob/master/minbpe/base.py#L13\n",
    "def get_stats(words):\n",
    "  word_counts = defaultdict(int)\n",
    "  for word in words:\n",
    "    for pair in zip(word, word[1:]):\n",
    "      word_counts[pair] += 1\n",
    "  return word_counts\n",
    "\n",
    "\n",
    "# do we need to remove it from vocab?? I don't think so but then how do we do it??\n",
    "# hmm we need to update the corpus after the merge of the tokens into the vocab\n",
    "# so the question is how do we set up the vocab in first place??\n",
    "def merge_most_frequent_tokens_and_get_them(stats, vocabulary):\n",
    "  tokens, freq = stats[0]\n",
    "  token1, token2 = tokens\n",
    "  vocabulary.add(token1+token2)\n",
    "  return token1, token2, token1+token2\n",
    "\n",
    "def update_corpus(tokenized_corpus, token1, token2, new_token):\n",
    "  new_corpus = []\n",
    "  for word in tokenized_corpus:\n",
    "    new_word = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "      # with len(word) - 1 we make sure that there are enough tokens in the future to look for\n",
    "      if i < len(word) - 1 and word[i] + word[i+1] == new_token: # these two tokens basically are forming the new_token\n",
    "        new_word.append(new_token)\n",
    "        i += 2 # skip the next token as it's merged\n",
    "      else:\n",
    "        new_word.append(word[i])\n",
    "        i += 1\n",
    "    new_corpus.append(new_word)\n",
    "  return new_corpus \n",
    "\n",
    "\n",
    "def bpe(corpus, k_merges=10):\n",
    "  #print(f\"corpus={corpus}\")\n",
    "  tokenized_corpus = [list(word) for word in corpus]\n",
    "  #print(f\"tokenized_corpus={tokenized_corpus}\")\n",
    "  # count the adjacent words and get the most frequent one\n",
    "  # we need  a matrix right?? but then this would become too much in size\n",
    "  vocabulary = get_unique_chars(tokenized_corpus)\n",
    "  #print(len(vocabulary))\n",
    "  #print(vocabulary)\n",
    "\n",
    "  for k in range(k_merges):\n",
    "    stats = get_stats(tokenized_corpus)\n",
    "    #print(stats)\n",
    "    # now sort the stats and get the most frequent occurrence\n",
    "    stats = list(sorted(stats.items(), key=lambda item: item[1], reverse=True))\n",
    "    #print(stats)\n",
    "\n",
    "    token1, token2, new_token = merge_most_frequent_tokens_and_get_them(stats, vocabulary)\n",
    "    #print(len(vocabulary))\n",
    "    #print(vocabulary)\n",
    "    print(f\"most frequent tokens are {token1, token2}\")\n",
    "\n",
    "    tokenized_corpus = update_corpus(tokenized_corpus, token1, token2, new_token)\n",
    "    #print(f\"updated_corpus={tokenized_corpus}\")\n",
    "    # update the corpus by replacing each occurrence of the 2 most adjancet frequencies with the new merged token, hmm we're doing the\n",
    "\n",
    "  return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most frequent tokens are ('\\\\', 'n')\n",
      "most frequent tokens are ('i', 'n')\n",
      "most frequent tokens are ('\\\\n', '\\\\n')\n",
      "most frequent tokens are ('e', 'r')\n",
      "most frequent tokens are ('o', 'n')\n",
      "most frequent tokens are ('e', 'n')\n",
      "most frequent tokens are ('e', 's')\n",
      "most frequent tokens are ('r', 'e')\n",
      "most frequent tokens are ('a', 'n')\n",
      "most frequent tokens are ('a', 'l')\n",
      "most frequent tokens are ('t', 'i')\n",
      "most frequent tokens are ('in', 'g')\n",
      "most frequent tokens are ('o', 'r')\n",
      "most frequent tokens are ('t', 'e')\n",
      "most frequent tokens are ('a', 'r')\n",
      "most frequent tokens are ('o', 'u')\n",
      "most frequent tokens are ('l', 'e')\n",
      "most frequent tokens are ('i', 't')\n",
      "most frequent tokens are ('.', '\\\\n\\\\n')\n",
      "most frequent tokens are ('e', 'l')\n",
      "most frequent tokens are ('s', 't')\n",
      "most frequent tokens are ('r', 'o')\n",
      "most frequent tokens are ('en', 't')\n",
      "most frequent tokens are ('ti', 'on')\n",
      "most frequent tokens are ('e', 'd')\n",
      "most frequent tokens are ('o', 'm')\n",
      "most frequent tokens are ('c', 'h')\n",
      "most frequent tokens are ('t', 'h')\n",
      "most frequent tokens are ('a', 'c')\n",
      "most frequent tokens are ('a', 's')\n",
      "most frequent tokens are ('i', 's')\n",
      "most frequent tokens are ('l', 'l')\n",
      "most frequent tokens are ('i', 'd')\n",
      "most frequent tokens are ('T', 'h')\n",
      "most frequent tokens are ('e', 'c')\n",
      "most frequent tokens are ('a', 't')\n",
      "most frequent tokens are ('a', 'm')\n",
      "most frequent tokens are ('i', 'c')\n",
      "most frequent tokens are ('s', ',')\n",
      "most frequent tokens are ('l', 'y')\n",
      "most frequent tokens are ('e', 't')\n",
      "most frequent tokens are ('r', 'i')\n",
      "most frequent tokens are ('a', 'd')\n",
      "most frequent tokens are ('s', 'i')\n",
      "most frequent tokens are ('a', 'tion')\n",
      "most frequent tokens are ('o', 'p')\n",
      "most frequent tokens are ('o', 'l')\n",
      "most frequent tokens are ('c', 'e')\n",
      "most frequent tokens are ('c', 'om')\n",
      "most frequent tokens are ('i', 'l')\n",
      "most frequent tokens are ('er', 's')\n",
      "most frequent tokens are ('es', 't')\n",
      "most frequent tokens are ('v', 'e')\n",
      "most frequent tokens are ('p', 'ro')\n",
      "most frequent tokens are ('u', 'n')\n",
      "most frequent tokens are ('e', 'v')\n",
      "most frequent tokens are ('a', 'te')\n",
      "most frequent tokens are ('c', 'on')\n",
      "most frequent tokens are ('u', 's')\n",
      "most frequent tokens are ('u', 'r')\n",
      "vocabulary after bpe = {'K', 'P', 'ðŸŽ®', '\\U000e0073', ';', 'id', 'o', '-', 'est', 'it', 'g', 'U', 'ðŸ“ž', 'Å‚', '9', 'te', 'ðŸ’¸', '\\U000e0074', 'X', 'k', 'ðŸ§°', 'ch', '\\U000e0063', 'com', 's,', 'Q', 'ðŸ‘‹', 'et', 'ate', 'H', 'b', 'ar', 'ï¸', 'ic', 'ed', 'st', 'd', 'en', 'ou', 'ce', 'â€¦', '\\\\n\\\\n', 'ly', 'a', 'ev', 'at', '\"', 'âš¡', 'q', 'h', 'r', 'B', 'op', 'R', 'F', 'or', '=', 'si', 'am', '\\U000e0062', 's', 'N', '3', '5', 'ðŸš€', 'V', 'ðŸ’¥', 'j', 'ðŸ´', 'er', 'ation', 'J', 'll', 'M', ',', 'â€“', 'â€™', '.\\\\n\\\\n', 'ðŸ“§', 'x', '2', '\\\\', 'âš™', 'ðŸ’³', 't', 'W', '.', 'Th', 'ðŸ ', '6', 'ol', 'A', 'us', 'ro', '0', 'm', 'ti', '(', 'in', 'ðŸ“ˆ', 'i', 'Ã©', '4', 'ðŸ’¼', 'v', 'âœ…', 'as', 'ðŸ¤‘', 'ðŸ¡', '@', '!', 'ur', 'L', 'âœ¨', 'I', '|', 'â™¨', '\\U000e0067', 'ers', 'l', 'â—', 'ðŸ“º', 'is', '8', '1', 'n', 're', 'C', \"'\", 'Y', 'D', '%', 'â€', 'on', 'ðŸ•¹', 'ing', 'ðŸ”¹', 'th', 'ad', '#', 'z', 'u', 'ðŸ', 'tion', 'ðŸ’Ž', 'ðŸ‘', 'il', 'ðŸ', 'es', ')', '&', '*', 'con', 'p', ':', '\\U000e007f', 'ðŸŒŽ', 'T', '?', 'un', 'an', '7', 'Â£', 'G', 'ðŸ‘', 'ec', 'pro', '\\\\n', 'le', 'O', 'â‚¬', 'ðŸ’°', 'el', '+', '$', 've', 'Z', 'f', 'ðŸ¤—', 'ðŸ˜œ', 'ac', 'E', 'y', 'S', '_', '/', 'â€”', 'â€¢', 'Ã¼', 'e', 'c', '~', 'w', '>', 'ri', 'al', 'om', 'ent'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = bpe(words, 60)\n",
    "print(f\"vocabulary after bpe = {vocabulary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
